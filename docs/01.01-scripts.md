Perfecto. Te prepararé un conjunto completo de scripts Bash y Python que:

* Crean automáticamente todos los recursos necesarios en AWS (Lambda, Step Functions, roles IAM, etc.)
* Usan un perfil AWS CLI (por defecto `default`, configurable con `-p`)
* Aceptan como parámetro el bucket de S3 donde se guardarán los Parquet
* Incluyen un `Dockerfile` y script para construir la imagen de Lambda con netCDF4, xarray, pandas, etc.
* Ejecutan el pipeline completo desde tu CLI local

Te avisaré cuando tenga todo listo para tu revisión.


# Implementación de arquitectura AWS para procesamiento de archivos NetCDF de MeteoGalicia a Parquet

## Descripción general de la solución

Esta solución despliega una arquitectura **serverless** en AWS para **descargar archivos NetCDF del catálogo THREDDS de MeteoGalicia**, transformarlos a formato **Parquet** y almacenarlos en **Amazon S3** para consulta con **Amazon Athena**. Los componentes principales son:

* **AWS Lambda**: Función de procesamiento (empaquetada como imagen Docker basada en Amazon Linux 2) que descarga un archivo NetCDF desde THREDDS, extrae las variables de interés (latitud, longitud, dirección y velocidad del viento) y guarda los datos como Parquet en S3.
* **AWS Step Functions**: Máquina de estado que orquesta la descarga y conversión para un rango de fechas, invocando la función Lambda de procesamiento para cada fecha en paralelo o secuencia.
* **Amazon S3**: Almacena los archivos Parquet generados. La ubicación de destino (bucket y prefijo) se pasa como parámetro.
* **AWS Glue / Athena (registro manual)**: tablas externas definidas vía DDL para que Athena consulte los Parquet. El pipeline ya no ejecuta crawlers automáticamente; si deseas catalogar los datos deberás lanzar las sentencias `CREATE EXTERNAL TABLE` / `MSCK REPAIR TABLE` por tu cuenta.
* **IAM Roles**: Roles y políticas necesarias para que Lambda y Step Functions tengan los permisos adecuados (acceso a S3, registro en CloudWatch, invocar Lambda, etc.).

**Diagrama de Flujo (Descripción)**: El flujo típico consiste en que el usuario inicia la ejecución del pipeline para un intervalo de fechas. Step Functions toma cada fecha y lanza la función Lambda de procesamiento, la cual descarga el NetCDF correspondiente desde MeteoGalicia (vía HTTP), lo procesa y escribe un Parquet en S3 (particionado por fecha). Una vez finalizado el procesamiento de todas las fechas, los datos pueden ser consultados en Athena tras definir la tabla externa mediante DDL.

## Preparativos y configuración inicial

Antes de implementar los scripts, asegúrate de tener configurado lo siguiente:

* **AWS CLI** instalado y configurado con credenciales válidas. Es recomendable crear un perfil de AWS CLI para este proyecto (por defecto usaremos el perfil `default`).
* **Docker** instalado y en ejecución, para construir la imagen de la función Lambda.
* **Permisos AWS apropiados**: el usuario/rol que ejecute estos scripts debe poder crear roles IAM, funciones Lambda, repositorios ECR, Step Functions y escribir en el bucket S3 de destino.
* Un **bucket de S3** creado para almacenar los datos Parquet (p.ej. `my-meteo-data-bucket`). Este nombre se proporcionará a los scripts como parámetro.
* (Opcional) Crear una **base de datos de Glue/Athena** (p.ej. `meteogalicia_db`) donde registrarás las tablas externas mediante DDL.

Todos los scripts Bash utilizarán el AWS CLI con un perfil especificado mediante `-p <perfil>` (por omisión `default`). Los comandos de AWS CLI incluirán `--profile $PROFILE` para operar en el perfil indicado. Asimismo, los scripts Python y la función Lambda asumen conexiones salientes a Internet (para acceder al servidor THREDDS de MeteoGalicia) y utilizarán librerías como **xarray**, **netCDF4**, **pandas**, **pyarrow** y **boto3**.

## Estructura recomendada del proyecto

Organiza los archivos en carpetas para mantener claridad entre código fuente, scripts de despliegue y otros recursos. Por ejemplo:

```plaintext
meteo-project/
├── lambda/                   # Código y definición de la función Lambda de procesamiento
│   ├── Dockerfile            # Dockerfile para construir la imagen de Lambda
│   └── lambda_processor.py   # Código Python de la función Lambda (procesamiento NetCDF->Parquet)
├── scripts/                  # Scripts Bash para crear recursos y ejecutar el pipeline
│   ├── create_roles.sh       # Crea roles IAM (Lambda y Step Functions)
│   ├── deploy_lambda.sh      # Construye la imagen Docker, crea repositorio ECR, sube la imagen y despliega la función Lambda
│   ├── create_state_machine.sh  # Crea la máquina de estados de Step Functions con la definición JSON
│   ├── run_pipeline.sh       # Inicia la ejecución del pipeline (Step Functions) para un rango de fechas dado
├── utils/                    # Scripts Python de utilidad
│   └── generate_urls.py      # Genera dinámicamente las URLs de NetCDF a descargar a partir de un intervalo de fechas
└── README.md                 # (Documentación del proyecto, instrucciones de uso)
```

A continuación, se proporcionan los contenidos de cada script y archivo, junto con explicaciones y ejemplos de uso.

## 1. Scripts Bash para despliegue de infraestructura y ejecución

### 1.1 `create_roles.sh`: Creación de roles IAM necesarios

Este script crea los roles IAM requeridos:

* **Rol de ejecución de Lambda** (`LambdaExecutionRole`): con confianza en el servicio Lambda y políticas para permitir escribir logs en CloudWatch y acceder al bucket S3 de destino.
* **Rol de Step Functions** (`StepFunctionsWorkflowRole`): con confianza en Step Functions y políticas para invocar la función Lambda de procesamiento.

El nombre del bucket de S3 se pasa con `-b <bucket>` para restringir las políticas de S3 a dicho bucket. El perfil de AWS CLI se especifica con `-p <perfil>`.

```bash
#!/bin/bash
# create_roles.sh - Crea roles IAM para Lambda y Step Functions.

PROFILE="default"
BUCKET=""
while getopts ":p:b:" opt; do
  case $opt in
    p) PROFILE=$OPTARG ;;
    b) BUCKET=$OPTARG ;;
    \?) echo "Uso: $0 [-p perfil] -b <bucket_S3_destino>"; exit 1 ;;
  esac
done

if [[ -z "$BUCKET" ]]; then
  echo "Error: Debe especificar el bucket de S3 destino con -b"
  exit 1
fi

# Obtener Account ID y Region para usar en ARNs
ACCOUNT_ID=$(aws sts get-caller-identity --profile "$PROFILE" --query Account --output text)
REGION=$(aws configure get region --profile "$PROFILE")
if [[ -z "$ACCOUNT_ID" || -z "$REGION" ]]; then
  echo "Error obteniendo Account ID o Region del perfil $PROFILE"
  exit 1
fi

echo "Creando rol de Lambda..."
# Política de confianza para Lambda (permite a Lambda asumir el rol)
read -r -d '' TRUST_LAMBDA << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": { "Service": "lambda.amazonaws.com" },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role --profile "$PROFILE" --role-name LambdaExecutionRole \
    --assume-role-policy-document "$TRUST_LAMBDA" \
    --description "Rol de ejecución de Lambda para procesamiento de MeteoGalicia"

# Adjuntar políticas administradas: logs de Lambda y XRay (para seguimiento) 
aws iam attach-role-policy --profile "$PROFILE" --role-name LambdaExecutionRole \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole 
aws iam attach-role-policy --profile "$PROFILE" --role-name LambdaExecutionRole \
    --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess 

# Política inline para permitir acceso de la Lambda al bucket S3 (get/put objetos en el bucket especificado)
read -r -d '' LAMBDA_S3_POLICY << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::$BUCKET/*",
        "arn:aws:s3:::$BUCKET"
      ]
    }
  ]
}
EOF

aws iam put-role-policy --profile "$PROFILE" --role-name LambdaExecutionRole \
    --policy-name LambdaS3AccessPolicy --policy-document "$LAMBDA_S3_POLICY"

echo "Rol de ejecución de Lambda creado y políticas adjuntadas."

echo "Creando rol de Step Functions..."
# Política de confianza para Step Functions
read -r -d '' TRUST_STEPFUNCTIONS << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": { "Service": "states.$REGION.amazonaws.com" },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role --profile "$PROFILE" --role-name StepFunctionsWorkflowRole \
    --assume-role-policy-document "$TRUST_STEPFUNCTIONS" \
    --description "Rol de Step Functions para orquestar el procesamiento MeteoGalicia"

# Política inline: permitir a Step Functions invocar cualquier función Lambda en esta cuenta (puede restringirse al ARN específico)
read -r -d '' SF_INVOKE_LAMBDA << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "lambda:InvokeFunction",
      "Resource": "arn:aws:lambda:$REGION:$ACCOUNT_ID:function:*"
    }
  ]
}
EOF

aws iam put-role-policy --profile "$PROFILE" --role-name StepFunctionsWorkflowRole \
    --policy-name SFInvokeLambdaPolicy --policy-document "$SF_INVOKE_LAMBDA"

echo "Rol de Step Functions creado y políticas adjuntadas."
echo "Todos los roles IAM necesarios se han creado correctamente."
```

**Explicación**: El script utiliza `aws iam create-role` para crear cada rol con su *trust policy*. Por ejemplo, el rol de Lambda permite que el servicio Lambda lo asuma. Luego adjuntamos políticas administradas: `AWSLambdaBasicExecutionRole` para permitir a la función Lambda escribir logs en CloudWatch Logs (y también X-Ray para rastreo, opcional). Adicionalmente, se agregan **políticas en línea** restringidas al bucket S3 de destino: la Lambda podrá `s3:PutObject`/`GetObject` en ese bucket y Step Functions podrá invocar funciones Lambda (a todas las funciones en la cuenta, aunque podría restringirse al ARN específico de la función de procesamiento).

> **Nota:** Para simplificar, en la política de Step Functions usamos `Resource: arn:aws:lambda:$REGION:$ACCOUNT_ID:function:*`, dando permiso para invocar cualquier Lambda en la cuenta y región. En un entorno de producción, se recomienda restringir al ARN exacto de la función Lambda de procesamiento (lo cual requeriría conocer el ARN/nombre de la función al crear la política, o actualizar la política después de crear la función).

### 1.2 `deploy_lambda.sh`: Construcción de imagen Docker y despliegue de la función Lambda

Este script automatiza la construcción de la imagen Docker de la función Lambda de procesamiento, su carga en Amazon ECR, y la creación/actualización de la función Lambda en AWS.

Toma como parámetros el perfil `-p`, el nombre del bucket `-b` (destino en S3), opcionalmente el nombre del repositorio ECR `-r` y el nombre de la función Lambda `-f`. Si no se especifican, usaremos valores por defecto (`meteogalicia-processor` para el repositorio, `MeteoGaliciaProcessor` para el nombre de la función). También se puede pasar un prefijo de S3 opcional con `-x` para ubicar los datos dentro del bucket (por defecto `meteo_parquet`).

```bash
#!/bin/bash
# deploy_lambda.sh - Construye la imagen Docker de la Lambda, la sube a ECR y crea/actualiza la función Lambda.

PROFILE="default"
BUCKET=""
REPO_NAME="meteogalicia-processor"
FUNCTION_NAME="MeteoGaliciaProcessor"
PREFIX="meteo_parquet"

while getopts ":p:b:r:f:x:" opt; do
  case $opt in
    p) PROFILE=$OPTARG ;;
    b) BUCKET=$OPTARG ;;
    r) REPO_NAME=$OPTARG ;;
    f) FUNCTION_NAME=$OPTARG ;;
    x) PREFIX=$OPTARG ;;
    \?) echo "Uso: $0 [-p perfil] -b <bucket> [-r repo_ecr] [-f nombre_funcion] [-x prefijo_s3]"; exit 1 ;;
  esac
done

if [[ -z "$BUCKET" ]]; then
  echo "Error: Debe especificar el bucket de S3 destino con -b"
  exit 1
fi

# Obtener Account ID y Region
ACCOUNT_ID=$(aws sts get-caller-identity --profile "$PROFILE" --query Account --output text)
REGION=$(aws configure get region --profile "$PROFILE")
REPO_URI="$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME"

echo "Iniciando construcción de la imagen Docker para la función Lambda..."
# Construir la imagen Docker localmente
docker build -t "$REPO_NAME:latest" ./lambda
if [[ $? -ne 0 ]]; then
  echo "Error: Falló la construcción de la imagen Docker"
  exit 1
fi

# Login a ECR
echo "Autenticando Docker con ECR..."
aws ecr get-login-password --region "$REGION" --profile "$PROFILE" | \
  docker login --username AWS --password-stdin "$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com"
if [[ $? -ne 0 ]]; then
  echo "Error: Falló la autenticación con Amazon ECR"
  exit 1
fi

# Crear repositorio ECR si no existe
echo "Creando repositorio ECR $REPO_NAME si no existe..."
aws ecr describe-repositories --repository-names "$REPO_NAME" --profile "$PROFILE" --region "$REGION" > /dev/null 2>&1
if [[ $? -ne 0 ]]; then
  aws ecr create-repository --repository-name "$REPO_NAME" --region "$REGION" --profile "$PROFILE" \
    --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE
  echo "Repositorio $REPO_NAME creado en ECR."
else
  echo "Repositorio $REPO_NAME ya existe en ECR."
fi

# Etiquetar la imagen local con el URI del repositorio ECR
docker tag "$REPO_NAME:latest" "$REPO_URI:latest"

# Push de la imagen a ECR
echo "Subiendo la imagen Docker a ECR..."
docker push "$REPO_URI:latest"
if [[ $? -ne 0 ]]; then
  echo "Error: Falló el push de la imagen a ECR"
  exit 1
fi
echo "Imagen subida a ECR: $REPO_URI:latest"

# Obtener ARN del rol de Lambda creado anteriormente
ROLE_ARN=$(aws iam get-role --role-name LambdaExecutionRole --profile "$PROFILE" --query Role.Arn --output text)
if [[ -z "$ROLE_ARN" ]]; then
  echo "Error: No se encontró el rol LambdaExecutionRole. Asegúrese de ejecutar create_roles.sh primero."
  exit 1
fi

# Determinar si la función Lambda ya existe
echo "Desplegando la función Lambda \"$FUNCTION_NAME\"..."
aws lambda get-function --function-name "$FUNCTION_NAME" --profile "$PROFILE" > /dev/null 2>&1
if [[ $? -ne 0 ]]; then
  # Crear función Lambda nueva
  aws lambda create-function --function-name "$FUNCTION_NAME" --package-type Image \
    --code ImageUri="$REPO_URI:latest" --role "$ROLE_ARN" --profile "$PROFILE" \
    --environment Variables="{DEST_BUCKET=$BUCKET,DEST_PREFIX=$PREFIX}" \
    --memory-size 1024 --timeout 300
  echo "Función Lambda creada: $FUNCTION_NAME"
else
  # Actualizar función Lambda existente (código e imagen)
  aws lambda update-function-code --function-name "$FUNCTION_NAME" \
    --image-uri "$REPO_URI:latest" --publish --profile "$PROFILE"
  # Actualizar configuración de entorno en caso de cambio de bucket/prefijo
  aws lambda update-function-configuration --function-name "$FUNCTION_NAME" \
    --environment Variables="{DEST_BUCKET=$BUCKET,DEST_PREFIX=$PREFIX}" \
    --memory-size 1024 --timeout 300 --profile "$PROFILE"
  echo "Función Lambda existente actualizada: $FUNCTION_NAME"
fi
```

**Explicación:** Este script empaqueta e implementa la función Lambda de procesamiento:

* Construye la imagen Docker a partir del Dockerfile en `./lambda`. La imagen incluye el runtime de Lambda Python y las dependencias necesarias (ver Dockerfile abajo).
* Se autentica en ECR y crea un repositorio (si no existe) para la imagen. Luego etiqueta la imagen local como `ECR_URI:latest` y la sube a ECR. Es importante incluir la etiqueta `:latest` en el URI al hacer push y al desplegar la Lambda.
* Obtiene el ARN del **LambdaExecutionRole** creado previamente, necesario para asignarlo a la función.
* Crea la función Lambda usando AWS CLI `aws lambda create-function --package-type Image ...` indicando la URI de imagen en ECR (con `:latest`) y el rol IAM. Configuramos variables de entorno `DEST_BUCKET` (bucket destino) y `DEST_PREFIX` (prefijo en S3 donde guardar los datos Parquet), así como memoria (1024 MB) y timeout (300 seg) adecuados para este procesamiento.
* Si la función ya existía, en lugar de crearla, usamos `aws lambda update-function-code` para apuntar a la nueva imagen (publicando una nueva versión) y `update-function-configuration` para ajustar variables de entorno, memoria o timeout según sea necesario.

> **Nota:** La imagen debe ser construida y subida para cada cambio de código en la función Lambda. Asegúrese de **incluir `:latest`** al final del URI de la imagen tanto al hacer push al repositorio ECR como al crear/actualizar la función Lambda, ya que Lambda utilizará ese tag para obtener la imagen específica. Los comandos de autenticación y creación de repositorio ECR se basan en las instrucciones oficiales de AWS.

**Dockerfile de la función Lambda (`lambda/Dockerfile`):** A continuación se muestra un Dockerfile de ejemplo que usa la imagen base de AWS Lambda para Python 3.9 (Amazon Linux 2) e instala las dependencias requeridas:

```Dockerfile
# Imagen base de AWS Lambda para Python 3.9 (Amazon Linux 2)
FROM public.ecr.aws/lambda/python:3.9

# Instalamos las dependencias necesarias (xarray, netCDF4, pandas, pyarrow, boto3, etc.)
# Usamos -U para asegurar la última versión compatible
RUN python3.9 -m pip install --upgrade --no-cache-dir \
    xarray netCDF4 pandas pyarrow boto3

# Copiamos el código fuente de la función Lambda al directorio de aplicación
COPY lambda_processor.py ${LAMBDA_TASK_ROOT}/

# Comando de entrada (handler) - no es necesario especificar si usamos la base AWS, 
# se puede definir en la configuración de la Lambda.
# Por claridad, definimos el handler: archivo .py y nombre de función handler
ENV AWS_LAMBDA_FUNCTION_HANDLER=lambda_processor.lambda_handler
```

**Puntos a notar:**

* Se utiliza la imagen pública **`public.ecr.aws/lambda/python:3.9`**, que ya incluye el runtime de Lambda para Python 3.9 sobre Amazon Linux 2. Esto simplifica el despliegue, ya que no necesitamos agregar el Runtime Interface Client manualmente. Solo instalamos las librerías requeridas dentro de la imagen.
* Las dependencias incluyen: **xarray**, **netCDF4** (para poder manejar archivos NetCDF), **pandas** y **pyarrow** (para crear Parquet), y **boto3** (para interacción con S3 desde el código). Todas se instalan vía pip.
* Se copia el archivo `lambda_processor.py` que contiene el código de la función. La variable de entorno `AWS_LAMBDA_FUNCTION_HANDLER` indica cuál es la función handler (en formato `<archivo>.<función>`). En este caso, esperamos que en `lambda_processor.py` exista una función `lambda_handler` que actúe como punto de entrada.
* No es necesario exponer puertos ni definir un CMD explícito ya que la base de AWS Lambda maneja la inicialización del runtime. Solo definimos el handler apropiado.

### 1.3 `create_state_machine.sh`: Creación de la máquina de estados Step Functions

Este script crea la máquina de estados de Step Functions que orquestará el pipeline. La definición del workflow se construye en JSON e incluye un estado *Map* para iterar sobre una lista de fechas/URLs, invocando la función Lambda de procesamiento para cada elemento.

Parámetros: perfil `-p`, nombre de la función Lambda si difiere del valor por defecto, y nombre a asignar a la máquina de estados `-n` (por defecto `MeteoGaliciaStateMachine`). Si la función Lambda se creó con el nombre por defecto (`MeteoGaliciaProcessor`), no es necesario especificar `-f`.

```bash
#!/bin/bash
# create_state_machine.sh - Crea la Step Functions State Machine para el pipeline de MeteoGalicia.

PROFILE="default"
STATE_MACHINE_NAME="MeteoGaliciaStateMachine"
FUNCTION_NAME="MeteoGaliciaProcessor"

while getopts ":p:n:f:" opt; do
  case $opt in
    p) PROFILE=$OPTARG ;;
    n) STATE_MACHINE_NAME=$OPTARG ;;
    f) FUNCTION_NAME=$OPTARG ;;
    \?) echo "Uso: $0 [-p perfil] [-n nombre_state_machine] [-f nombre_funcion_lambda]"; exit 1 ;;
  esac
done

# Obtener ARNs necesarios
ACCOUNT_ID=$(aws sts get-caller-identity --profile "$PROFILE" --query Account --output text)
REGION=$(aws configure get region --profile "$PROFILE")
LAMBDA_ARN="arn:aws:lambda:$REGION:$ACCOUNT_ID:function:$FUNCTION_NAME"
ROLE_ARN=$(aws iam get-role --role-name StepFunctionsWorkflowRole --profile "$PROFILE" --query Role.Arn --output text)

if [[ -z "$ROLE_ARN" ]]; then
  echo "Error: Rol StepFunctionsWorkflowRole no encontrado. Ejecute create_roles.sh primero."
  exit 1
fi

echo "Creando definición de la máquina de estados Step Functions..."

# Definición JSON de la máquina de estados (usando aqui-heredoc para insertar ARN de Lambda dinámicamente)
read -r -d '' STATE_DEF << EOF
{
  "Comment": "State machine para procesar archivos NetCDF de MeteoGalicia en rango de fechas",
  "StartAt": "ProcesarTodasLasFechas",
  "States": {
    "ProcesarTodasLasFechas": {
      "Type": "Map",
      "ItemsPath": "\$.urlList",
      "MaxConcurrency": 2,
      "Iterator": {
        "StartAt": "ProcesarUnaFecha",
        "States": {
          "ProcesarUnaFecha": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
              "FunctionName": "$LAMBDA_ARN",
              "Payload": {
                "url.\$": "\$\$.Map.Item.Value"
              }
            },
            "End": true
          }
        }
      },
      "End": true
    }
  }
}
EOF

# Crear la máquina de estado
aws stepfunctions create-state-machine --name "$STATE_MACHINE_NAME" --definition "$STATE_DEF" \
  --role-arn "$ROLE_ARN" --type STANDARD --profile "$PROFILE"

echo "Máquina de estados '$STATE_MACHINE_NAME' creada exitosamente."
```

**Explicación:** Este script prepara la definición JSON del workflow de Step Functions e invoca `aws stepfunctions create-state-machine` para crearla.

* La **máquina de estados** se llama por defecto "MeteoGaliciaStateMachine" (puedes cambiar con `-n`). Es de tipo STANDARD (ejecución duradera).
* Obtenemos el ARN de la función Lambda de procesamiento (con nombre `$FUNCTION_NAME`) para referenciarla en la definición. También obtenemos el ARN del rol StepFunctionsWorkflowRole creado antes, que se asignará como rol de la máquina (Step Functions lo asume para ejecutar las tareas).
* La definición (`STATE_DEF`) contiene un estado **Map** llamado "ProcesarTodasLasFechas". Este estado toma un array de URLs ubicado en `$.urlList` del input y crea iteraciones para cada elemento.

  * Usamos `"MaxConcurrency": 2` para limitar a 2 lambdas concurrentes (esto es ajustable; se puede aumentar para paralelizar más procesamiento si la cuenta/región lo permite, o poner 1 para secuencial).
  * Cada iteración invoca una tarea Lambda "ProcesarUnaFecha" usando la integración directa `arn:aws:states:::lambda:invoke`. En **Parameters**, pasamos el nombre de la función (`FunctionName`) con el ARN completo, y definimos el `Payload` que recibirá la Lambda: `{"url": "<valor>"}`. Aquí utilizamos la sintaxis `$.Map.Item.Value` para insertar el elemento actual del array como el campo `url` en el evento de Lambda.
* El Map termina cuando todas las lambdas han procesado sus elementos. Actualmente, el flujo **finaliza** ahí (`End: true`). Si se deseara encadenar un paso adicional (por ejemplo, notificar vía SNS o lanzar un proceso separado para registrar la tabla en Athena), se podría añadir otro estado después de *ProcesarTodasLasFechas*.

El rol `StepFunctionsWorkflowRole` es referenciado en la creación (parámetro `--role-arn`). Este rol contiene permisos para invocar funciones Lambda, según lo definimos en `create_roles.sh`. Step Functions usará esos permisos durante la ejecución del workflow.

> **Nota:** Estamos pasando una lista de **URLs completas** al Step Function (clave `urlList`). Alternativamente, podríamos pasar solo fechas y construir las URLs dentro de la Lambda. Aquí optamos por generar las URLs fuera para flexibilidad. El código Lambda aceptará un campo `url` en el evento. Asegúrate de que el nombre de la clave en la definición (`url` en `Payload`) coincida exactamente con lo que la Lambda espera.

### 1.4 `run_pipeline.sh`: Ejecución del pipeline sobre un rango de fechas

Este script inicia una **ejecución** de la máquina de estados de Step Functions, proporcionando como entrada el rango de fechas deseado. Internamente, utiliza el script Python `generate_urls.py` para construir la lista de URLs a descargar para esas fechas, y luego llama a `aws stepfunctions start-execution`.

Parameters: `-p <profile>`, `-s <start_date>`, `-e <end_date>` in `YYYY-MM-DD` format (e.g. `-s 2025-01-01 -e 2025-01-05`). Optional: `-n <state_machine_name>` if not using the default. To ingest specific regions, repeat `-g <boundary.geojson> -r <region_name>` for each area (region_name defaults to the geojson filename if omitted).

```bash
#!/bin/bash
# run_pipeline.sh - Inicia la ejecución del pipeline Step Functions para un rango de fechas dado.

PROFILE="default"
STATE_MACHINE_NAME="MeteoGaliciaStateMachine"
START_DATE=""
END_DATE=""
GEOJSON=""

while getopts ":p:n:s:e:g:" opt; do
  case $opt in
    p) PROFILE=$OPTARG ;;
    n) STATE_MACHINE_NAME=$OPTARG ;;
    s) START_DATE=$OPTARG ;;
    e) END_DATE=$OPTARG ;;
    g) GEOJSON=$OPTARG ;;
    \?) echo "Uso: $0 [-p perfil] [-n nombre_state_machine] -s YYYY-MM-DD -e YYYY-MM-DD [-g boundary.geojson]"; exit 1 ;;
  esac
done

if [[ -z "$START_DATE" || -z "$END_DATE" ]]; then
  echo "Error: Debe especificar fecha de inicio (-s) y fin (-e) en formato YYYY-MM-DD"
  exit 1
fi

# Obtener ARN de la state machine
STATE_MACHINE_ARN=$(aws stepfunctions list-state-machines --profile "$PROFILE" --query "stateMachines[?name=='$STATE_MACHINE_NAME'].stateMachineArn" --output text)
if [[ -z "$STATE_MACHINE_ARN" ]]; then
  echo "Error: Máquina de estados '$STATE_MACHINE_NAME' no encontrada. Asegúrese de haberla creado con create_state_machine.sh"
  exit 1
fi

# Generar el JSON de input con lista de URLs usando el script Python
echo "Generando lista de URLs de NetCDF desde $START_DATE hasta $END_DATE..."
if [[ -n "$GEOJSON" ]]; then
  INPUT_JSON=$(python utils/generate_urls.py "$START_DATE" "$END_DATE" "$GEOJSON")
else
  INPUT_JSON=$(python utils/generate_urls.py "$START_DATE" "$END_DATE")
fi
if [[ $? -ne 0 || -z "$INPUT_JSON" ]]; then
  echo "Error generando la lista de URLs o leyendo GeoJSON. Verifique los parámetros."
  exit 1
fi

# Iniciar la ejecución del Step Function con el JSON generado
echo "Iniciando ejecución de Step Functions..."
EXECUTION_ARN=$(aws stepfunctions start-execution --state-machine-arn "$STATE_MACHINE_ARN" \
               --input "$INPUT_JSON" --profile "$PROFILE" --query executionArn --output text)

if [[ -z "$EXECUTION_ARN" ]]; then
  echo "Error: No se pudo iniciar la ejecución de la máquina de estados."
  exit 1
fi

echo "Ejecución iniciada exitosamente. ARN de la ejecución: $EXECUTION_ARN"
echo "Puede monitorear el estado de la ejecución en la consola de Step Functions o usando AWS CLI."
```

**Explicación:** Este script es sencillo:

* Obtiene el ARN de la state machine por nombre (usando `list-state-machines` y filtrando por nombre).
* Llama al script Python `generate_urls.py` (ver sección siguiente) pasándole la fecha de inicio y fin. Ese script imprime un JSON con la lista de URLs. Capturamos esa salida en `INPUT_JSON`.
* Usamos `aws stepfunctions start-execution` para iniciar el workflow, pasando `--input "$INPUT_JSON"` directamente. Gracias a que `INPUT_JSON` es una cadena JSON bien formada con comillas escapadas apropiadamente, el CLI acepta el parámetro (alternativamente, podríamos guardar en un archivo y usar `--input file://...`).
* Mostramos el ARN de la ejecución iniciada para su seguimiento.

El JSON de entrada tendrá el formato:
```json
{
  "urlList": [
    "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_HIST/d03/2025/01/wrf_arw_det_history_d03_20250101_0000.nc4",
    "... (una por día hasta la fecha fin) ..."
  ],
  "polygon": [ [lon1, lat1], [lon2, lat2], ... ]  # (opcional si se proporcionó GeoJSON)
}
```
La máquina de estado espera la clave `urlList` según definimos en `ItemsPath`. Si se incluye `polygon`, la Lambda de procesamiento filtrará los puntos al área definida.

> **Nota:** Se asume que las fechas proporcionadas corresponden a archivos NetCDF existentes en el catálogo THREDDS de MeteoGalicia. En caso de solicitar fechas fuera del rango disponible o inexistentes, la Lambda podría fallar al no encontrar la URL (devolverá error HTTP 404). Se puede mejorar el código para manejar esas situaciones (p.ej., capturando códigos HTTP y omitiendo archivos no encontrados), pero en esta primera versión no se incluye dicha lógica.


## 2. Scripts Python de utilidad y código de la función Lambda

A continuación se detallan los scripts Python solicitados:

### 2.1 `generate_urls.py`: dynamic MeteoGalicia URL generation

This helper builds the `urlList` that seeds the Step Functions execution. It now exposes a `-m/--model` flag so the operator can decide at run time which MeteoGalicia domain should be downloaded. The flag accepts the following values (default: `wrf4km`):

| Flag value | Description | Path pattern |
| --- | --- | --- |
| `wrf4km` | `WRF_HIST` domain d03 (~4 km) | `https://.../WRF_HIST/d03/<YYYY>/<MM>/wrf_arw_det_history_d03_<YYYYMMDD>_0000.nc4` |
| `wrf1_3km` | `WRF_ARW_1KM_HIST` domain d05 (1.3 km) | `https://.../WRF_ARW_1KM_HIST/d05/<YYYY>/<MM>/wrf_arw_det1km_history_d05_<YYYYMMDD>_0000.nc4` |
| `wrf1km` | `WRF_ARW_1KM_HIST_Novo` domain d02 (1 km) | `https://.../WRF_ARW_1KM_HIST_Novo/<YYYYMMDD>/wrf_arw_det_history_d02_<YYYYMMDD>_0000.nc4` |

The excerpt below highlights the new model dictionary and CLI parsing (the repository version also handles optional boundary polygons and `test_points` CSVs):

```python
#!/usr/bin/env python3
import csv
import json
import os
import sys
from datetime import datetime, timedelta

MODEL_CONFIGS = {
    "wrf4km": {
        "base_url": "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_HIST/d03",
        "filename_template": "wrf_arw_det_history_d03_{date}_0000.nc4",
        "folder_style": "year_month",
    },
    "wrf1_3km": {
        "base_url": "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_ARW_1KM_HIST/d05",
        "filename_template": "wrf_arw_det1km_history_d05_{date}_0000.nc4",
        "folder_style": "year_month",
    },
    "wrf1km": {
        "base_url": "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_ARW_1KM_HIST_Novo",
        "filename_template": "wrf_arw_det_history_d02_{date}_0000.nc4",
        "folder_style": "yyyymmdd",
    },
}
DEFAULT_MODEL = "wrf4km"

def _extract_model_option(argv):
    model = DEFAULT_MODEL
    remaining = []
    i = 0
    while i < len(argv):
        arg = argv[i]
        if arg in ("-m", "--model"):
            if i + 1 >= len(argv):
                raise SystemExit("Missing argument for -m/--model")
            model = argv[i + 1].lower()
            i += 2
        elif arg.startswith("--model="):
            model = arg.split("=", 1)[1].lower()
            i += 1
        else:
            remaining.append(arg)
            i += 1
    return model, remaining

def _build_url(model_key, date_obj):
    cfg = MODEL_CONFIGS[model_key]
    date_str = date_obj.strftime("%Y%m%d")
    filename = cfg["filename_template"].format(date=date_str)
    if cfg["folder_style"] == "year_month":
        return f"{cfg['base_url']}/{date_obj:%Y}/{date_obj:%m}/{filename}"
    return f"{cfg['base_url']}/{date_str}/{filename}"

model_choice, positional_args = _extract_model_option(sys.argv[1:])
start_date = datetime.strptime(positional_args[0], "%Y-%m-%d")
end_date = datetime.strptime(positional_args[1], "%Y-%m-%d")

urls = []
current = start_date
while current <= end_date:
    urls.append(_build_url(model_choice, current))
    current += timedelta(days=1)

output = {"urlList": urls, "source_model": model_choice}
print(json.dumps(output))
```

Key points:

* `MODEL_CONFIGS` centralises the base URL, filename template, and folder layout for each MeteoGalicia product.
* `_extract_model_option` strips any `-m/--model` flags before parsing the required `start` and `end` dates, so the rest of the logic can treat boundaries and CSV arguments transparently.
* `_build_url` understands the per-model folder convention (`YYYY/MM` vs `YYYYMMDD`) and builds the final HTTP URL.
* The JSON payload now includes `"source_model"` so downstream jobs know which resolution was requested.

Example invocation filtered to the 1 km dataset:

```bash
$ python utils/generate_urls.py -m wrf1km 2025-01-01 2025-01-03
{"urlList": [
  "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_ARW_1KM_HIST_Novo/20250101/wrf_arw_det_history_d02_20250101_0000.nc4",
  "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_ARW_1KM_HIST_Novo/20250102/wrf_arw_det_history_d02_20250102_0000.nc4",
  "https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_ARW_1KM_HIST_Novo/20250103/wrf_arw_det_history_d02_20250103_0000.nc4"
], "source_model": "wrf1km"}
```

`scripts/04-run_pipeline.sh` forwards the flag automatically, so operators can keep using a single entry point for Step Functions executions.

#### Tracking the MeteoGalicia source model downstream

The `source_model` emitted by `utils/generate_urls.py` is now propagated end-to-end so every artefact records the MeteoGalicia domain that produced it:

1. **Step Functions input** &mdash; `scripts/03-create_state_machine.sh` injects `source_model` into the Map state's `Parameters`. You can confirm the value that seeded a run with:

   ```bash
   aws stepfunctions describe-execution \
     --execution-arn <execution_arn> \
     --query 'input.source_model'
   ```

2. **Lambda payload and GeoParquet rows** &mdash; `lambda/lambda_processor.py` receives the field, copies it into every record as the `source_model` column, and includes the same value inside the per-hour sidecar. Use the mandatory DuckDB container for quick inspections without downloading the entire dataset:

   ```bash
   docker run --rm -v "$PWD":/data -w /data duckdb/duckdb:latest \
     -c "SELECT source_model, COUNT(*) FROM 'local_sync/year=2025/month=01/day=01/hour=00/data.parquet' GROUP BY 1;"
   ```

   To read the sidecar metadata:

   ```bash
   jq '.source_model' local_sync_metadata/year=2025/month=01/day=01/hour=00/metadata.json
   ```

3. **STAC catalogues** &mdash; `scripts/11-build_stac_catalog.py` copies the column into each Item under `properties.source_model` and aggregates the distinct values at Collection level via `"source_models"`. Once the catalog is generated you can query any dataset provenance with:

   ```bash
   jq -r '.properties.source_model' catalogs/<collection>/items/parquet/<item_id>/item.json
   jq -r '.source_models' catalogs/<collection>/collection.json
   ```

These hooks make it straightforward to audit which MeteoGalicia resolution (4 km, 1.3 km, or 1 km) was used across Step Functions runs, hourly GeoParquet partitions, and the published STAC catalogue.

### 2.2 `lambda_processor.py`: Código de la función Lambda para descargar NetCDF, convertir a Parquet y subir a S3

Este es el **código Python** que corre dentro de la función Lambda (el archivo que copiamos en la imagen Docker). Implementa la lógica de tomar una URL (o una fecha), descargar el archivo NetCDF, extraer las variables requeridas y producir un archivo Parquet en S3.

```python
import os
import re
import tempfile
import xarray as xr
import pandas as pd
import boto3

# Variables de entorno proporcionadas en la configuración de Lambda
DEST_BUCKET = os.environ.get("DEST_BUCKET", "")
DEST_PREFIX = os.environ.get("DEST_PREFIX", "").rstrip("/")  # quitar '/' final si existe

s3_client = boto3.client('s3')

def lambda_handler(event, context):
    """
    Handler de Lambda que procesa un archivo NetCDF:
      - Descarga el archivo desde la URL proporcionada (o construida a partir de una fecha).
      - Convierte las variables lat, lon, dir (dirección viento) y mod (velocidad viento) a un DataFrame tabular.
      - Escribe el DataFrame a un archivo Parquet.
      - Sube el Parquet al bucket S3 destino (organizado por particiones year, month, day).
    """
    # Obtener URL de NetCDF del evento
    url = None
    date_str = None
    if "url" in event:
        url = event["url"]
        # Intentar extraer la fecha YYYYMMDD del nombre de archivo en la URL (asumiendo el patrón conocido)
        m = re.search(r"_([0-9]{8})_0000.nc4$", url)
        if m:
            date_str = m.group(1)  # Ejemplo: "20250101"
        else:
            # Si no se puede extraer, no se define date_str (se podría manejar distinto según naming)
            date_str = None
    elif "date" in event:
        # Si en lugar de URL se provee una fecha, construir la URL (misma lógica que generate_urls.py)
        date_input = str(event["date"])
        try:
            # Formato esperado 'YYYY-MM-DD'
            dt = pd.to_datetime(date_input)
            date_str = dt.strftime("%Y%m%d")
            year = dt.strftime("%Y")
            month = dt.strftime("%m")
            filename = f"wrf_arw_det_history_d03_{date_str}_0000.nc4"
            url = f"https://mandeo.meteogalicia.es/thredds/fileServer/modelos/WRF_HIST/d03/{year}/{month}/{filename}"
        except Exception as e:
            raise ValueError(f"Formato de fecha no válido: {date_input}") from e
    else:
        raise ValueError("El evento no contiene 'url' ni 'date' para procesar.")

    # Definir ruta temporal para descargar el NetCDF
    local_netcdf = os.path.join(tempfile.gettempdir(), "data.nc")
    try:
        import requests
        resp = requests.get(url, timeout=60)
        resp.raise_for_status()
        with open(local_netcdf, "wb") as f:
            f.write(resp.content)
    except Exception as e:
        # Registrar error y abortar
        print(f"ERROR: Falló la descarga del archivo {url}. Detalle: {e}")
        raise

    # Abrir el archivo NetCDF con xarray
    try:
        ds = xr.open_dataset(local_netcdf)
    except Exception as e:
        print(f"ERROR: No se pudo abrir el archivo NetCDF descargado. Detalle: {e}")
        raise

    # Verificar que las variables necesarias existan en el dataset
    for var in ["lat", "lon", "dir", "mod"]:
        if var not in ds.variables:
            print(f"ERROR: La variable '{var}' no se encuentra en el dataset")
            raise KeyError(f"Variable {var} ausente en los datos")

    # Extraer variables necesarias
    # lat, lon son 2D (dims: y, x); dir, mod son 3D (dims: time, y, x)
    lat = ds["lat"].values   # shape (y, x)
    lon = ds["lon"].values   # shape (y, x)
    wind_dir = ds["dir"].values  # shape (time, y, x)
    wind_spd = ds["mod"].values  # shape (time, y, x)
    times = ds["time"].values    # shape (time,)

    # Obtener dimensiones
    num_times = times.shape[0]
    ny, nx = lat.shape
    num_points = ny * nx

    # Convertir a DataFrame tabular
    # Aplanamos las matrices 2D y 3D en 1D. Asumimos que la memoria está organizada por [time, y, x] en wind_dir/spd.
    # Creamos arrays 1D para cada columna:
    # Repetimos cada marca temporal 'num_points' veces para alinear con todos los puntos (lat,lon) de ese tiempo.
    # Repetimos la cuadrícula lat/lon para cada instante temporal.
    time_col = pd.Series(pd.to_datetime(times).repeat(num_points), name="time")
    lat_col = pd.Series(lat.flatten()).repeat(num_times).reset_index(drop=True).rename("lat")
    lon_col = pd.Series(lon.flatten()).repeat(num_times).reset_index(drop=True).rename("lon")
    # Flatten de las variables de viento (que deberían alinearse con el orden de time repeat y lat/lon tile)
    wind_dir_col = pd.Series(wind_dir.reshape(-1), name="wind_dir")
    wind_spd_col = pd.Series(wind_spd.reshape(-1), name="wind_speed")

    # Combinar en un DataFrame
    df = pd.DataFrame({
        "time": time_col,
        "lat": lat_col,
        "lon": lon_col,
        "wind_dir": wind_dir_col,
        "wind_speed": wind_spd_col
    })

    # Opcional: filtrar o manipular datos (p.ej., convertir direcciones NaN o out-of-range, etc.)
    # En este caso asumimos que los datos ya vienen limpios.

    # Definir ruta de archivo GeoParquet temporal
    local_parquet = os.path.join(tempfile.gettempdir(), "data.parquet")
    # Save GeoDataFrame as GeoParquet, promoting ingestion metadata to columns
    try:
        import geopandas as gpd
        # Convertimos a GeoDataFrame usando lon/lat y definimos la proyección geográfica estándar (WGS84).
        # Las columnas 'x' y 'y' permanecen en unidades del modelo (por ej. km) bajo su CRS original.
        # Para trabajar directamente en coordenadas 'x','y' será necesario reproyectar usando
        # el CRS original almacenado en el sidecar JSON.
        gdf = gpd.GeoDataFrame(
            df, geometry=gpd.points_from_xy(df.lon, df.lat), crs="EPSG:4326"
        )
        # Serialize geometry to WKB and promote only partition-driving metadata columns
        gdf['geom_wkb'] = gdf.geometry.apply(lambda geom: geom.wkb)
        df_for_table = gdf.drop(columns=["lat", "lon", "geometry"]).rename(columns={"geom_wkb": "geometry"})
        df_for_table["date"] = time_col.dt.strftime("%Y-%m-%d")
        df_for_table["hour"] = time_col.dt.strftime("%H")
        df_for_table["timestamp"] = time_col.dt.strftime("%Y-%m-%dT%H:%M:%SZ")

        table = pa.Table.from_pandas(df_for_table, preserve_index=False)
        # Escribir GeoParquet con compresión snappy
        pq.write_table(table, local_parquet, compression="snappy")

        # Persist NetCDF attributes and external lineage in a dedicated sidecar JSON
        metadata_sidecar = {
            "crs": proj_string,
            "nc_proj_string": proj_string,
            "netcdf_attributes": ds.attrs
        }
        if regions:
            metadata_sidecar["regions"] = regions
        if test_points:
            metadata_sidecar["test_points"] = test_points
        if url:
            metadata_sidecar["source_url"] = url
        local_metadata = os.path.join(tempfile.gettempdir(), "metadata.json")
        with open(local_metadata, "w", encoding="utf-8") as fh:
            json.dump(metadata_sidecar, fh, default=str, indent=2)
    except Exception as e:
        print(f"ERROR: Falló la escritura a GeoParquet. Detalle: {e}")
        raise

    # Construir la clave de S3 de destino
    # Usaremos particionado por fecha del run (date_str si está disponible, sino tomamos del primer timestamp)
    if date_str is None:
        # Si no se extrajo date_str, derivar de la primera marca de tiempo 'time'
        if not df.empty:
            date_str = pd.to_datetime(df["time"].iloc[0]).strftime("%Y%m%d")
        else:
            date_str = "unknown"
    year = date_str[0:4]
    month = date_str[4:6]
    day = date_str[6:8]

    prefix = DEST_PREFIX + "/" if DEST_PREFIX else ""
    key = f"{prefix}year={year}/month={month}/day={day}/data.parquet"

    # Subir el Parquet a S3
    try:
        s3_client.upload_file(local_parquet, DEST_BUCKET, key)
        print(f"Archivo Parquet subido a s3://{DEST_BUCKET}/{key}")
    except Exception as e:
        print(f"ERROR: No se pudo subir el archivo Parquet a S3. Detalle: {e}")
        raise

    return {
        "status": "OK",
        "bucket": DEST_BUCKET,
        "key": key,
        "record_count": len(df)
    }
```

**Explicación del código Lambda:**

* **Entrada (event):** El handler soporta dos modos:

  * Si `event` contiene `"url"`, usa ese URL directamente.
  * Si contiene `"date"`, construye la URL a partir de la fecha (mismo método que `generate_urls.py`). Esto permite invocar la Lambda con solo la fecha. En nuestro Step Function usamos URLs, pero se dejó compatibilidad por flexibilidad.
* Descarga el archivo NetCDF desde la URL usando `requests`. Escribe el contenido a un archivo temporal en `/tmp` (Lambda tiene 512 MB de `/tmp` disponible por defecto, suficiente para \~66 MB). Se maneja excepción si falla la descarga (por timeout, 404, etc.).
* Abre el dataset con **xarray** (`xr.open_dataset`). Esto mapea las variables NetCDF a un `Dataset` de xarray con dimensiones. Se verifica que existan las variables requeridas: **lat**, **lon**, **dir** (dirección del viento a 10 m) y **mod** (módulo o velocidad del viento a 10 m). Estas variables son conocidas del modelo WRF de MeteoGalicia.

  * *lat* y *lon* son arrays 2D (latitud y longitud en cada punto de la rejilla).
  * *dir* y *mod* son arrays 3D con dimensiones (tiempo, y, x).
* Extraemos los valores numpy de cada variable. Luego procedemos a **reordenarlos en un DataFrame tabular**:

  * Calculamos `num_times` (número de timestamps), `ny, nx` (tamaño de la grilla espacial) y `num_points = ny*nx`.
  * Creamos la columna de tiempo repitiendo cada timestamp tantas veces como puntos espaciales (esto hace que cada instante aparezca en múltiples filas, una por cada ubicación espacial).
  * Repetimos los arrays de latitud y longitud para cada instante de tiempo. Aquí se optó por usar `pandas.Series.repeat` para duplicar eficientemente los valores. Alternativamente, se podría usar `numpy.tile` o `np.repeat`. El resultado es que `lat_col` y `lon_col` contienen la latitud/longitud de cada fila en sincronía con las demás columnas.
  * Aplanamos (`reshape(-1)`) los arrays 3D de `wind_dir` y `wind_spd` en 1D. **Importante:** Dado que por defecto xarray abre con dimensión order (time, y, x), el flatten resultante recorre primero la dimensión más a la derecha (x), luego y, luego time. Repetir las latitudes seguidas por tiempos como se hizo debe alinear correctamente: primero se listan todos los puntos para tiempo0, luego todos los puntos para tiempo1, etc. Se ha construido las columnas de esa manera (tiempos repetidos en bloques, lat/lon repetidos totalmente por cada tiempo) para que cada valor de wind\_dir/spd se corresponda con su lat, lon y timestamp en la misma fila.
  * Unimos todas las columnas en un DataFrame `df` con las columnas: `time, lat, lon, wind_dir, wind_speed`. No incluimos índices (reset index) para que quede como tabla simple.
  * Opcionalmente podríamos filtrar datos inválidos (por ejemplo, si *lat, lon* tenían fill-values, o *dir, mod* contienen nulos), pero supondremos que están limpios. Si hubiera `_FillValue` en NetCDF, xarray normalmente los manejará como NaN.
* Write the GeoDataFrame to a **GeoParquet** file in `/tmp` using `pyarrow`. The geometry is stored in lat/lon (WGS84) while the ingestion lineage (`date`, `hour`, `timestamp`) lives in dedicated columns. Projection, region definitions, optional test points, and the original source URL are persisted exclusively in the sidecar JSON located at `metadata/<dataset>/year=YYYY/month=MM/day=DD/hour=HH/metadata.json`, together with the NetCDF attributes.
* Construimos la **clave S3** donde se guardará el Parquet. Usamos variables de entorno:

  * `DEST_BUCKET` ya está definido.
  * `DEST_PREFIX` puede ser cadena vacía o un prefijo de carpeta. Añadimos `/` si tiene valor.
  * Usamos el `date_str` (YYYYMMDD) para crear subdirectorios `year=YYYY/month=MM/day=DD/`. Si `date_str` no se obtuvo (por ejemplo, si la URL no siguió el patrón o se proporcionó solo una fecha sin parsear correctamente), intentamos derivarlo del primer timestamp del DataFrame. En última instancia, si no hay timestamp (df vacío), se usa "unknown".
  * Nombramos el archivo Parquet simplemente como `data.parquet` dentro del directorio de partición. Cada ejecución por fecha típica generará un archivo por día en su correspondiente carpeta.
* Finalmente, subimos el archivo Parquet a S3 con `boto3.client.upload_file`. Si falla, lanzamos excepción. En caso de éxito, imprimimos un mensaje de confirmación con la ruta S3.
* La función retorna un dict con algunos metadatos útiles (`status`, bucket, key, número de registros procesados), aunque en este caso Step Functions no lo usa explícitamente. Podría ser útil para debug o log.

**Detalles importantes:**

* **Memoria**: El proceso de convertir a DataFrame y luego a Parquet puede usar bastante memoria, especialmente si el dominio WRF es grande. Asignamos 1024 MB a la Lambda. Xarray carga lazy por defecto, pero al acceder a `.values` forzamos la carga completa en memoria de esas arrays. Una posible optimización es procesar en bloques (por ejemplo, escribir por cada tiempo en Parquet incrementally), pero para simplicidad esta versión lo hace de una sola vez. Si se prevén archivos muy grandes, se podría aumentar la memoria de la Lambda o optimizar el código.
* **Tiempo de ejecución**: La descarga de \~50-100 MB vía internet, más el procesamiento, podría llevar varios segundos por archivo. Con 1024 MB de memoria, la CPU de Lambda es proporcional (\~equivalente a 1 vCPU), lo que debería manejarlo en quizás 10-30 segundos por archivo (dependiendo del tamaño y eficiencia de pyarrow). El timeout se estableció en 300s (5 min) para margen. Con concurrency=2 en Step Functions, el pipeline procesará 2 días a la vez.
* **Dirección del viento (dir)**: Asumimos que en los NetCDF de MeteoGalicia la variable `dir` ya representa la dirección del viento en grados y `mod` la velocidad (magnitud) en las 10m. Esto nos evita calcular dirección a partir de componentes U/V. (En la lista de variables NCSS se veía `u` y `v` por separado, pero también `dir` y `mod` pre-calculados). En caso contrario, hubiéramos convertido U/V a dirección con `atan2`.
* **Coordenadas lat/lon**: Incluimos lat y lon en cada fila para poder consultar fácilmente por ubicación en Athena. La redundancia de lat/lon en cada registro aumenta el tamaño, pero dado que Parquet comprime eficientemente columnas con muchos valores repetidos, es aceptable. Otra estrategia podría ser guardar solo indices de grilla y mantener un archivo separado de coordenadas, pero complicaría las consultas en Athena. Aquí privilegiamos la *desnormalización* para simplicidad de consulta.
* **Formato de tiempo**: Athena reconoce datos de tiempo si son tipo timestamp en Parquet. Nuestra columna "time" proviene de xarray, que suele ser `numpy.datetime64[ns]` convertida a pandas `datetime64[ns]`. `to_parquet` con pyarrow conservará el tipo timestamp (posiblemente como *int64 with timestamp metadata*). Asegurémonos de que se maneje correctamente. Por precaución, convertimos `times` a pandas datetime via `pd.to_datetime` antes de repeat, así garantizamos que es timezone-naive timestamp (Athena tratará como UTC probablemente).
### Working with model grid coordinates (x, y)

The GeoParquet file stores its geometry in geographic coordinates (longitude/latitude, EPSG:4326) for compatibility with GIS tools. The original model grid coordinates (`x`, `y`) remain as columns, while the projection string and NetCDF lineage are exposed via the external metadata sidecar.

To perform analyses in the model grid space (e.g. distances in kilometers or grid-based transforms), reconstruct or reproject the GeoDataFrame using the original CRS:

```python
import json
import geopandas as gpd
import pyarrow.parquet as pq

# Read the GeoParquet file
table = pq.read_table("data.parquet")
df = table.to_pandas()
sidecar = json.load(open("metadata/.../metadata.json", "r", encoding="utf-8"))
original_proj = sidecar.get("nc_proj_string") or sidecar.get("crs")
gdf = gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_wkb(df["geometry"]), crs="EPSG:4326")

# Reproject to model grid space (x, y in km)
gdf_model = gdf.to_crs(original_proj)

# Now gdf_model.geometry.x and .y correspond to model grid coordinates in kilometers
```

## 3. Ejemplos de ejecución de los scripts

A modo de demostración, a continuación se muestran los pasos para desplegar y ejecutar todo el pipeline usando los scripts proporcionados. Suponiendo que hemos colocado los scripts y archivos según la estructura antes mencionada:

1. **Crear IAM roles** (ejecutar una vez al inicio):

   ```bash
   $ chmod +x scripts/create_roles.sh
   $ scripts/create_roles.sh -p default -b my-meteo-data-bucket
   ```

   Esto creará `LambdaExecutionRole` y `StepFunctionsWorkflowRole` con las políticas necesarias. Verifique en la consola IAM o via CLI que los roles existen y contienen las políticas esperadas.

2. **Construir y desplegar la función Lambda**:

   ```bash
   $ chmod +x scripts/deploy_lambda.sh
   $ scripts/deploy_lambda.sh -p default -b my-meteo-data-bucket
   ```

   Este comando compilará la imagen Docker, la subirá a ECR y creará la función Lambda llamada "MeteoGaliciaProcessor". Puede personalizar el nombre de la función o repositorio con `-f` y `-r` si desea. Si todo va bien, verá mensajes indicando la creación o actualización de la función. En AWS Lambda, debería aparecer la función con el rol adecuado y la imagen referenciada.

3. **Crear la máquina de estados Step Functions**:

   ```bash
   $ chmod +x scripts/create_state_machine.sh
   $ scripts/create_state_machine.sh -p default
   ```

   Esto tomará el ARN de la función Lambda creada y creará una state machine llamada "MeteoGaliciaStateMachine". Tras ejecutarse, puede comprobar en la consola de Step Functions que existe y ver su diagrama (debería mostrar un Map state dentro). Asegúrese de que el rol asociado es `StepFunctionsWorkflowRole`. La definición JSON generada incluye el ARN completo de la Lambda de procesamiento.

4. **Registrar la tabla externa en Athena (manual)**:

   ```bash
   aws athena start-query-execution \
     --profile default \
     --query-string "CREATE EXTERNAL TABLE IF NOT EXISTS meteogalicia_db.wind_data (\n       lat DOUBLE,\n       lon DOUBLE,\n       wind_dir DOUBLE,\n       wind_speed DOUBLE,\n       timestamp TIMESTAMP\n     )\n     PARTITIONED BY (year INT, month INT, day INT)\n     STORED AS PARQUET\n     LOCATION 's3://my-meteo-data-bucket/meteo_parquet/'" \
     --result-configuration OutputLocation=s3://my-meteo-data-bucket/athena-results/
   ```

   Después de crear la tabla ejecuta `MSCK REPAIR TABLE meteogalicia_db.wind_data;` para descubrir las particiones `year=/month=/day=/`. Desde este momento Athena conocerá la estructura sin necesidad de ningún crawler.


5. **Ejecutar el pipeline para un rango de fechas**:

   ```bash
   $ chmod +x scripts/run_pipeline.sh
   $ scripts/run_pipeline.sh -p default -s 2025-01-01 -e 2025-01-03
   ```

   Este comando iniciará la máquina de estados para las fechas 1 a 3 de enero de 2025. Internamente generará las URLs:

   * 2025-01-01 -> wrf\_arw\_det\_history\_d03\_20250101\_0000.nc4
   * 2025-01-02 -> wrf\_arw\_det\_history\_d03\_20250102\_0000.nc4
   * 2025-01-03 -> wrf\_arw\_det\_history\_d03\_20250103\_0000.nc4
     y lanzará la ejecución. El script imprimirá el ARN de la ejecución Step Functions. Puede monitorear el progreso con:

   ```bash
   aws stepfunctions describe-execution --execution-arn <arn_devuelto> --profile default
   ```

   Este comando mostrará el `status` (RUNNING, SUCCEEDED, etc.). También, en la consola web de Step Functions podrá ver cada iteración del Map y sus resultados en tiempo real. Cuando termine, debería indicar `SUCCEEDED`.

6. **Verificar archivos en S3**: Después de la ejecución exitosa, revise el bucket S3 (`my-meteo-data-bucket`). Deberían existir carpetas para cada fecha procesada. Siguiendo el ejemplo anterior:

   ```
   s3://my-meteo-data-bucket/meteo_parquet/year=2025/month=01/day=01/data.parquet
   s3://my-meteo-data-bucket/meteo_parquet/year=2025/month=01/day=02/data.parquet
   s3://my-meteo-data-bucket/meteo_parquet/year=2025/month=01/day=03/data.parquet
   ```

   Cada archivo Parquet contiene las columnas mencionadas con todos los puntos de la rejilla para todas las horas disponibles en ese archivo NetCDF original. Por ejemplo, si cada NetCDF tenía 73 horas (3 días pronóstico cada hora) en una grilla de 100x100 puntos (10,000 puntos), el Parquet tendrá 73*100*100 = 73,000 registros (aprox), con muchas repeticiones en lat/lon (lo que Parquet comprime bien).

7. **Consultar con Athena**: Una vez registrada la tabla con las sentencias anteriores, abre Athena, selecciona la base de datos (por ej. *meteogalicia\_db*) y ejecuta consultas SQL. Ejemplos:

   * Contar registros:

     ```sql
     SELECT COUNT(*) FROM meteogalicia_db.meteo_parquet;
     ```
   * Ver algunos datos:

     ```sql
     SELECT time, lat, lon, wind_dir, wind_speed 
     FROM meteogalicia_db.meteo_parquet 
     WHERE year=2025 AND month='01' AND day='01' 
     LIMIT 10;
     ```
   * Consultar velocidad promedio de viento en una ubicación aproximada:

     ```sql
     SELECT AVG(wind_speed) as avg_wspd 
     FROM meteogalicia_db.meteo_parquet 
     WHERE lat BETWEEN 42.0 AND 42.5 
       AND lon BETWEEN -8.5 AND -8.0;
     ```

   (Asumiendo que lat \~42N, lon \~-8W está dentro del dominio, esta consulta promediaría velocidades sobre un área de Galicia).

   Athena debería poder leer los timestamps y números correctamente. Recuerda que **Athena distingue mayúsculas/minúsculas en nombres de columnas** según cómo definiste la tabla; en nuestro caso probablemente todas minúsculas.

8. **Automatización adicional**: Puedes programar ejecuciones del Step Function diariamente (p.ej. con EventBridge Scheduler) para incorporar nuevos datos y ejecutar un script propio que lance las sentencias DDL/`MSCK REPAIR TABLE` tras cada ingestión. También puedes implementar alertas en caso de fallo de la Lambda (ej. mediante CloudWatch Alarms on Lambda errors).

## Consideraciones finales

* **Ejecución desde CLI**: Toda la solución se despliega y ejecuta vía CLI/scripts, sin requerir intervención manual en la consola web. Asegúrese de tener el perfil AWS CLI correctamente autenticado. Los ejemplos usan `-p default` pero puede ser cualquier otro perfil configurado.
* **Limpieza de recursos**: Si ya no necesita esta infraestructura, recuerde eliminar los componentes para evitar cargos innecesarios. Puede usar AWS CLI para eliminar la función Lambda (`aws lambda delete-function`), la state machine (`aws stepfunctions delete-state-machine`), los roles IAM (con `aws iam delete-role` tras quitar políticas) y los datos en S3. El ECR repo puede ser eliminado con `aws ecr delete-repository` (asegurando primero que esté vacío con `aws ecr batch-delete-image` si es necesario).
* **Costos**: Esta solución utiliza servicios serverless con costo bajo por uso. Lambda se cobra por milisegundos de ejecución y memoria (una ejecución procesando \~50MB puede costar unos pocos centavos a lo sumo). Step Functions Standard tiene costo por estado transicionado (cada fecha procesada cuenta como varios estados: uno en Map + lambda invoke). Athena tiene costos por volumen de datos escaneados. Mantener datos Parquet en S3 también incurre en costo de almacenamiento, pero es relativamente económico (además Parquet está comprimido). Todos estos costos deben ser mínimos para volúmenes moderados de datos, pero es bueno monitorear el gasto si se procesan rangos muy amplios.
* **Escalabilidad**: Se puede ajustar la concurrencia del Map en Step Functions para procesar más días en paralelo si la cuenta lo permite, o aumentar la memoria de Lambda para acelerar el procesamiento (CPU proporcional). En pruebas iniciales, se recomienda empezar con pocos días para validar la funcionalidad y luego expandir.
* **Depuración**: En caso de errores, consulte CloudWatch Logs. La función Lambda (por la política AWSLambdaBasicExecutionRole) crea logs por cada ejecución. Los prints de error implementados ayudarán a identificar en qué paso pudo fallar (descarga, procesamiento, subida S3, etc.).

Con estos scripts y consideraciones, contamos con una solución completa que **automatiza la extracción de datos meteorológicos** de MeteoGalicia y los prepara para análisis en la nube con Athena. ¡Ahora es posible consultar fácilmente grandes volúmenes de datos de viento a través de SQL, gracias a la conversión a Parquet optimizado y la potencia de AWS Athena!
