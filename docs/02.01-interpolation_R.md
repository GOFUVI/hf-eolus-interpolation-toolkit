Perfecto. Investigaré cómo identificar las variables UTM (x, y en km) en los NetCDF de Meteogalicia, y generaré los scripts R y Bash necesarios para:

* Leer los datos desde S3 (Parquet con dirección y velocidad)
* Convertir a componentes u y v
* Interpolar u y v mediante IDW y Regression-Kriging (usando R con los paquetes `phylin`, `gstat`, `Metrics`)
* Generar la nueva malla en resolución de kilómetros (parámetro de entrada)
* Calcular latitud y longitud a partir de x, y interpolados
* Guardar los resultados en Parquet en S3
* Proporcionar scripts Bash que provisionen los permisos necesarios (roles, policies) usando la CLI de AWS con opción `-p` para seleccionar perfil, y por defecto `default`, región `eu-west-3`

Te aviso en cuanto lo tenga listo para revisión.


# Flujo de interpolación de viento de Meteogalicia (NetCDF a Parquet a resultados)

## 1. Preparación de datos (coordenadas UTM y grilla de salida)

En primer lugar, necesitamos identificar las coordenadas en **proyección UTM** proporcionadas por los datos de MeteoGalicia. Los ficheros NetCDF incluyen normalmente variables de coordenadas `x` e `y` (en kilómetros) que representan la posición de cada punto de malla en el sistema UTM de la zona correspondiente. Por ejemplo, para Galicia se utiliza UTM zona 29N (WGS84, EPSG:32629). Estas variables `x` e `y` se pueden extraer del NetCDF mediante herramientas como **ncdf4** en R, obteniendo vectores (o matrices) de coordenadas Este (x) y Norte (y) en kilómetros.

Una vez identificadas las coordenadas, definimos la **malla de salida** con la resolución deseada. La resolución (en km) será un parámetro de usuario (por ejemplo, 1 km, 5 km, etc.). Usaremos los mínimos y máximos de `x` e `y` del NetCDF (o de los Parquet) para delimitar el dominio. La grilla de salida se genera con `seq(min, max, by = resolución)` para `x` y `y`, formando un grid regular en coordenadas UTM. Por ejemplo, si `min(x)=500 km`, `max(x)=600 km` y la resolución pedida es 5 km, generaremos puntos en 500, 505, 510, … hasta 600 km en **ambas** direcciones, y construiremos todas las combinaciones (parrilla regular). Cada punto de esta grilla tendrá coordenadas UTM (x, y) donde se interpolarán los datos.

**Nota:** Es importante asegurarse de que las coordenadas y la proyección se manejan correctamente. La proyección UTM zona 29N tiene unidades en metros; sin embargo, en el NetCDF de MeteoGalicia es probable que `x` e `y` estén expresadas en kilómetros (por ejemplo, 550 corresponde a 550 km Este). Conviene confirmar esta información en los metadatos del NetCDF o documentación asociada. Si estuvieran en km, al convertir a metros habrá que multiplicar por 1000 al realizar transformaciones de coordenadas (p.ej., a lat/long).

## 2. Conversión de dirección y velocidad a componentes U/V

Los datos originales incluyen **dirección del viento** (en grados) y **velocidad del viento** (en, por ejemplo, m/s). Para realizar una interpolación espacial correcta, primero convertiremos estas variables a sus componentes cartesianas: **componente *u*** (este-oeste) y **componente *v*** (norte-sur). Esto evita problemas con la naturaleza circular de la dirección (0° ≈ 360°) y permite interpolar magnitudes continuas.

Usaremos la convención meteorológica estándar: la *dirección* suele darse como el origen del viento (0° = viento del norte, 90° = viento del este, etc.). Las fórmulas para convertir a componentes son:

* `u = -V * sin(θ)`
* `v = -V * cos(θ)`

donde `V` es la velocidad del viento y `θ` es la dirección en radianes. Estas fórmulas aseguran que, por ejemplo, un viento del norte (θ = 0°) produzca `u = 0` y `v < 0` (viento hacia el sur, componente norte-sur negativa), o un viento del este (θ = 90°) produzca `u < 0` (viento hacia el oeste) y `v = 0`. En R, aplicaremos estas conversiones antes de cualquier interpolación, generando dos columnas nuevas `u` y `v` a partir de la dirección y velocidad originales.

## 3. Interpolación espacial (IDW y Regression-Kriging)

Una vez que tenemos las componentes `u` y `v` en cada punto conocido (los puntos de la malla original del modelo), procedemos a interpolarlas a los puntos de la **nueva grilla**. Implementaremos dos métodos de interpolación distintos:

### Interpolación IDW (Inverse Distance Weighting)

La interpolación IDW asigna a cada punto desconocido un valor promedio ponderado de los puntos conocidos, donde los pesos son el inverso de alguna potencia de la distancia. Usaremos el paquete `phylin` en R, que ofrece la función `idw()` para este propósito. Según la documentación, `phylin::idw` espera: (a) un vector de valores a interpolar, (b) un conjunto de coordenadas de los puntos de origen, y (c) las coordenadas de los puntos destino (grilla). Por defecto utiliza el método de Shepard con potencia `p=2` (IDW cuadrático inverso).

En nuestro caso, aplicaremos `idw` por separado a `u` y a `v`. Es decir, interpolemos la componente `u` conocida en todos los puntos de entrada hacia todos los puntos de la grilla de salida, y lo mismo con `v`. Obtendremos así `u_idw` y `v_idw` para cada punto de la grilla. Se puede mantener el parámetro por defecto `p=2` o ajustarlo según convenga; también se podría limitar el número de vecinos (`N`) o el radio de influencia (`R`) si se desea (parámetros opcionales de `idw()`).

**Implementación con `phylin`:** Suponiendo que `datos` es un data.frame con columnas `x, y, u, v`, y `grid` es otro data.frame con columnas `x, y` de la malla de salida, el uso sería por ejemplo:

```r
library(phylin)
# Interpolación IDW de u:
u_idw_vals <- idw(values = datos$u, coords = datos[, c("x","y")], grid = grid[, c("x","y")])
# Interpolación IDW de v:
v_idw_vals <- idw(values = datos$v, coords = datos[, c("x","y")], grid = grid[, c("x","y")])
```

La salida `u_idw_vals` y `v_idw_vals` serán vectores de valores interpolados correspondientes, en el **mismo orden** de puntos que `grid`. Luego podemos anexar estos resultados al data.frame de la grilla.

### Interpolación por Regression-Kriging (Kriging con deriva)

El **regression-kriging** combina una regresión (modelo determinístico) con kriging de los residuos. En nuestro caso, podemos implementar kriging universal donde la **deriva (tendencia)** se modela en función de las coordenadas, mientras que los **residuos** (variación espacial no explicada por la tendencia) se interpolan mediante kriging. Utilizaremos el paquete `gstat` para esto, que proporciona funciones para ajustar variogramas y hacer predicciones kriging.

**Pasos para regression-kriging:**

1. **Modelo de regresión:** Ajustamos un modelo inicial para la variable con respecto a la posición. Si no tenemos otras variables explicativas (como altitud u otras), podemos usar una tendencia plana o un plano en función de `x` e `y`. Por ejemplo, para la componente `u` ajustaremos `u ~ x + y` (una superficie inclinada), y similar para `v ~ x + y`. Esto capta gradientes globales en el campo de viento.

2. **Variograma de residuos:** Calculamos los residuos del modelo anterior y analizamos su estructura espacial. Con `gstat` podemos calcular directamente el variograma residual utilizando la fórmula con deriva. Por ejemplo, `variogram(u ~ x + y, data)` nos devuelve el variograma de `u` tras remover la tendencia lineal de `x,y`. Ajustamos un modelo teórico a ese variograma empírico usando `fit.variogram`. Por ejemplo, elegimos un modelo esférico (`vgm("Sph")`) u otro adecuado, y `fit.variogram` estimará los parámetros (sill, rango, nugget) óptimos.

3. **Kriging universal:** Utilizamos la función `krige` de `gstat` para predecir en la grilla destino. Especificamos la fórmula con la deriva y el modelo de variograma ajustado. De este modo, `krige(u ~ x+y, datos, nueva_grilla, model = modelo_variograma)` produce las predicciones krigeadas de `u` en los puntos de `nueva_grilla` teniendo en cuenta la tendencia `x+y` y el variograma de los residuos. Repetimos el proceso para `v`. Este procedimiento devuelve típicamente un objeto spatial (SpatialPointsDataFrame o sf) con columnas `var1.pred` (predicción) y `var1.var` (varianza de kriging) para cada punto. Nos centraremos en `var1.pred` como valor estimado.

En resumen, obtendremos `u_krig` y `v_krig` para cada punto de la grilla. Estos incorporan una tendencia global y la correlación espacial de los datos.

**Nota:** A diferencia de IDW (determinística), el kriging requiere suponer una estructura de correlación (variograma). Es importante verificar que el variograma ajustado represente bien los datos. En caso de datos escasos o muy irregulares, la regresión lineal en función de `x,y` podría no ser significativa; aun así, el método se reduce a kriging ordinario si la deriva es constante (\~1). Para simplificar, hemos usado `x,y` como covariables; si se dispusiera de otras variables (e.g. elevación), podrían incorporarse de forma similar (kriging con deriva externa).

## 4. Cálculo de RSR y sesgo (BIAS)

Para evaluar y comparar la calidad de las interpolaciones IDW vs Kriging, calcularemos la **razón RSR (RMSE-Standard Deviation Ratio)** y el **sesgo medio (BIAS)** mediante validación cruzada. Emplearemos el paquete `Metrics` de R para calcular RMSE y bias, y luego normalizaremos el RMSE dividiéndolo por la desviación estándar de los valores observados. Por convención, `bias` calcula el promedio de *(valor\_observado - valor\_predicho)*. Un sesgo cercano a 0 indica que no hay desviación sistemática (las sobreestimaciones y subestimaciones se compensan).

Realizaremos una **validación cruzada tipo leave-one-out** sobre los puntos originales del modelo:

* Para **IDW**: usando la función `krige.cv` de **gstat** podemos realizar LOOCV especificando una interpolación IDW. Aunque `krige.cv` se diseñó para kriging, permite indicar `set = list(idp = X)` para usar IDW en la validación. De este modo obtenemos, para cada punto original, una predicción basada en todos los demás. Alternativamente, podríamos manualmente iterar quitando cada punto y aplicando `phylin::idw` al resto; sin embargo, `krige.cv` simplifica el proceso.

* Para **Kriging**: igualmente usamos `krige.cv` con la fórmula y variograma. Esto nos dará las predicciones cruzadas en cada estación, omitiéndose a sí misma en la estimación.

Con los resultados de la validación cruzada, obtenemos listas de valores observados vs predichos para `u` y para `v` en cada método. Aplicamos `Metrics::rmse(obs, pred) / sd(obs)` para calcular RSR y `Metrics::bias(obs, pred)` para obtener el sesgo de cada componente y método. Idealmente, un buen método tendrá RSR bajo y bias cercano a 0.

Podemos reportar, por ejemplo: *"RMSE\_IDW\_u = X, RMSE\_IDW\_v = Y, RMSE\_Krig\_u = ..., etc"* y lo mismo para bias. Esto cuantificará qué método interpoló con menor error promedio y si tienden a sobrestimar o subestimar.

## 5. Conversión de coordenadas UTM a lat/long

Una vez obtenidas las predicciones en coordenadas UTM, es útil convertir esas coordenadas a **latitud/longitud geográficas** para integrar los resultados con otros sistemas de referencia (por ejemplo, visualizar en mapas). Utilizaremos el paquete **sf** o **sp** en R para realizar esta proyección.

Con **sf** es muy sencillo: definimos un objeto `sf` a partir del data.frame de resultados indicando su CRS actual (UTM 29N). Luego usamos `st_transform` al CRS WGS84 (EPSG:4326) que nos dará geometrías en lon/lat. Finalmente extraemos esas coordenadas transformadas y las añadimos como columnas `lat` y `lon`.

En R, por ejemplo:

```r
library(sf)
result_sf <- st_as_sf(result_df, coords = c("x", "y"), crs = 32629)  # UTM 29N WGS84
result_latlon <- st_transform(result_sf, 4326)
# Extraer columnas lat/long:
coords_latlon <- st_coordinates(result_latlon)
result_df$lon <- coords_latlon[,1]
result_df$lat <- coords_latlon[,2]
```

Ahora cada fila de resultados tiene sus coordenadas tanto en UTM (x,y) como en latitud/longitud (lon, lat), facilitando su interpretación geográfica.

## 6. Almacenamiento de resultados en Parquet (S3 particionado por fecha)

El resultado final de la interpolación (componentes u/v interpoladas, y posiblemente también velocidad/dirección recalculadas) se guardará en **formato Parquet**, almacenado en un bucket S3. Organizamos los archivos en particiones por fecha para un acceso más eficiente. Esto significa que para cada fecha de ejecución (por ejemplo, cada fecha de pronóstico) habrá un directorio o prefijo separado en S3, permitiendo leer fácilmente múltiples fechas mediante **partition pruning**.

Una estrategia común es incluir la fecha en la ruta o como parte del nombre de fichero. Usando la librería **arrow** en R, podemos hacer esto automáticamente. Por ejemplo, si añadimos una columna `fecha` al data frame de resultados (tipo `Date` o string `YYYY-MM-DD`), podemos usar `write_dataset()` indicando `partitioning = "fecha"`. Esto creará una carpeta por cada valor único de `fecha` y colocará allí el Parquet correspondiente. Por ejemplo: `s3://<bucket>/resultados/fecha=2025-05-14/part-...parquet`.

Al guardar en S3, nos aseguramos de tener las credenciales/roles apropiados (ver sección de AWS abajo). Podemos guardar directamente desde R a `s3://...` si la configuración AWS lo permite (arrow lo soporta), o bien guardar localmente y luego usar AWS CLI para subir. En este flujo asumiremos escritura directa a S3 para automatización.

**Estructura de datos final:** Cada archivo Parquet de resultados contendrá columnas como:

* `fecha` (partición)
* `x, y` (coordenadas UTM en km),
* `lon, lat` (coordenadas geográficas),
* `u_idw, v_idw` (componentes interpolados por IDW),
* `u_krig, v_krig` (componentes interpolados por kriging),
* opcionalmente `vel_idw, dir_idw, vel_krig, dir_krig` si se decide reconvertir a velocidad (magnitude) y dirección (en grados) las componentes interpoladas.

De este modo, los usuarios pueden consultar el Parquet para obtener el campo de viento interpolado por el método deseado en cada punto de la malla final, en la fecha correspondiente.

## 7. Scripts R para la ejecución del flujo

A continuación se proveen los scripts R necesarios para llevar a cabo todo el flujo de trabajo descrito. Están diseñados para correr por línea de comandos (usando `Rscript`).

### Script R: Conversión de NetCDF a Parquet (opcional)

Si bien los datos NetCDF ya se han convertido a Parquet en S3, incluimos un script de ejemplo para la **conversión** en caso de ser necesario procesar nuevos NetCDF. Este script lee un NetCDF local, extrae las variables requeridas (coordenadas, velocidad y dirección del viento), convierte a componentes u/v y guarda un archivo Parquet. Puede adaptarse si el NetCDF contiene múltiples tiempos o niveles.

```r
# convert_netcdf_to_parquet.R
# Instalar/usar paquetes necesarios
library(ncdf4)
library(arrow)

# Parámetros de entrada/salida
nc_file <- "wrf_arw_det_history_d03_20250514_0000.nc4"  # NetCDF de ejemplo
output_parquet <- "wind_20250514_0000.parquet"

# 1. Abrir NetCDF y leer variables
nc <- nc_open(nc_file)
# Suponiendo nombres de variable de viento (ejemplo, ajustar según NetCDF real):
wind_speed <- ncvar_get(nc, "wind_speed")  # matriz 2D (dimensiones: y, x) o 3D (t, y, x)
wind_dir   <- ncvar_get(nc, "wind_dir")    # misma dimensión que wind_speed
x_vals <- ncvar_get(nc, "x")              # vector de coordenadas X (km)
y_vals <- ncvar_get(nc, "y")              # vector de coordenadas Y (km)
# Si el NetCDF tiene dimensión temporal:
# time_vals <- ncvar_get(nc, "time")  # opcional, no usado aquí

# 2. Preparar datos en forma tabular
# Suponemos una sola capa temporal por simplicidad. Construimos todas las combinaciones de puntos.
grid_coords <- expand.grid(x = x_vals, y = y_vals)  # todas las parejas (x,y)
# Los datos de viento pueden venir como matriz [length(y) x length(x)]
grid_coords$velocidad <- as.vector(wind_speed)  # aplanar la matriz en un vector
grid_coords$direccion <- as.vector(wind_dir)

# 3. Convertir dirección/velocidad a componentes u, v
deg2rad <- pi/180
grid_coords$u <- - grid_coords$velocidad * sin(grid_coords$direccion * deg2rad)
grid_coords$v <- - grid_coords$velocidad * cos(grid_coords$direccion * deg2rad)

# 4. Guardar a Parquet
write_parquet(grid_coords, output_parquet)
nc_close(nc)
```

*Comentarios:* Este script asume nombres de variable (`"wind_speed"`, `"wind_dir"`) y dimensiones sencillas. En la práctica, los nombres podrían ser otros (por ejemplo, `WS`/`WD` o similares). Habría que ajustarlos según el NetCDF de MeteoGalicia. También, si existen múltiples tiempos, se podría añadir una columna de fecha/tiempo y particionar el Parquet por tiempo.

### Script R: Interpolación de viento (IDW & Kriging)

Este es el script principal que toma los datos de entrada ya en Parquet (posiblemente almacenados en S3), realiza la interpolación con ambos métodos, calcula métricas y guarda los resultados en Parquet en S3. Se asume que el script recibirá parámetros por la línea de comandos: fecha a procesar, resolución de grilla, ruta de input y ruta de output.

```r
# wind_interpolation.R
# Carga de librerías necesarias
library(arrow)    # para leer/escribir Parquet y S3
library(phylin)   # para IDW
library(gstat)    # para kriging
library(sf)       # para transformar coordenadas
library(Metrics)  # para calcular RMSE y bias

# Leer argumentos de entrada (fecha, resolución, rutas S3)
args <- commandArgs(trailingOnly = TRUE)
if(length(args) < 7) {
  stop(
    "Uso: Rscript wind_interpolation.R <fecha> <res_km> <input_path> <output_path> \
<cutoff_km> <width_km> <subsample_pct>"
  )
}
fecha_str   <- args[1]             # e.g. "2025-05-14"
res_km      <- as.numeric(args[2]) # e.g. 5 (km)
input_path  <- args[3]             # e.g. "s3://mi-bucket/datos_parquet/"
output_path <- args[4]             # e.g. "s3://mi-bucket/resultados/"
cutoff_km   <- as.numeric(args[5]) # max variogram distance km
width_km    <- as.numeric(args[6]) # variogram bin width km
subsample_pct <- as.numeric(args[7]) # percentage of points to subsample (of total data)

# 1. Leer datos de entrada desde Parquet (filtrando por fecha si aplica)
# Suponemos que input_path contiene Parquets posiblemente particionados por fecha.
# Si los datos tienen columna 'fecha' podemos filtrar; si cada archivo es una fecha, ajustar ruta.
message("Leyendo datos de entrada de ", input_path)
ds <- open_dataset(input_path)
# Filtrar por fecha si hay columna 'fecha' o similar:
# data <- ds %>% filter(fecha == fecha_str) %>% collect()
# Si no hay partición por fecha, se podría omitir el filtro (leer todo o seleccionar por nombre de archivo).
data <- collect(ds)  # (En este ejemplo simple, leemos todo y luego filtramos en R si es necesario)
if("fecha" %in% names(data)) {
  data <- subset(data, fecha == fecha_str)
}
n_pts <- nrow(data)
message("Puntos originales cargados: ", n_pts)

# 2. Asegurarse de que existen columnas u y v (si no, calcularlas)
if(!("u" %in% names(data) && "v" %in% names(data))) {
  message("Calculando componentes u/v a partir de velocidad/dirección...")
  deg2rad <- pi/180
  data$u <- - data$velocidad * sin(data$direccion * deg2rad)
  data$v <- - data$velocidad * cos(data$direccion * deg2rad)
}

# 3. Construir la malla de salida con la resolución indicada
min_x <- floor(min(data$x)); max_x <- ceiling(max(data$x))
min_y <- floor(min(data$y)); max_y <- ceiling(max(data$y))
# Generar secuencias con paso = res_km (en km)
grid_x <- seq(min_x, max_x, by = res_km)
grid_y <- seq(min_y, max_y, by = res_km)
grid_coords <- expand.grid(x = grid_x, y = grid_y)
message("Malla de salida generada: ", nrow(grid_coords), " puntos (res = ", res_km, " km)")

# 4. Interpolación IDW para u y v
message("Interpolando con IDW...")
idw_u <- idw(values = data$u, coords = data[, c("x","y")], grid = grid_coords)  # vector resultado
idw_v <- idw(values = data$v, coords = data[, c("x","y")], grid = grid_coords)
# Añadir resultados IDW al data.frame de la grilla
grid_coords$u_idw <- idw_u
grid_coords$v_idw <- idw_v

# 5. Interpolación por Regression-Kriging (usando gstat)
message("Ajustando variogramas y ejecutando kriging...")
# Convertir datos a objeto spatial (sp o sf) para gstat
library(sp)
coordinates(data) <- ~ x + y
# Asumimos proyección UTM 29N (EPSG:32629)
proj4string(data) <- CRS("+proj=utm +zone=29 +datum=WGS84 +units=km +no_defs")
# Crear objeto spatial para la grilla
gridded_points <- SpatialPoints(grid_coords[, c("x","y")], proj4string = CRS(proj4string(data)))

# Variograma y kriging para u
vgm_u <- variogram(u ~ x + y, data)                     # variograma de residuos de u
model_u <- fit.variogram(vgm_u, vgm(model="Sph"))       # ajustar modelo (esférico inicial)
krig_u <- krige(u ~ x + y, data, newdata = gridded_points, model = model_u)
grid_coords$u_krig <- krig_u$var1.pred  # extraer predicción de u

# Variograma y kriging para v
vgm_v <- variogram(v ~ x + y, data)
model_v <- fit.variogram(vgm_v, vgm(model="Sph"))
krig_v <- krige(v ~ x + y, data, newdata = gridded_points, model = model_v)
grid_coords$v_krig <- krig_v$var1.pred

# 6. Calcular métricas de error (LOOCV RMSE y bias)
message("Calculando RMSE y bias por validación cruzada...")
# Validación cruzada IDW utilizando krige.cv con idp:
cv_idw_u <- krige.cv(u ~ 1, data, nfold = n_pts, set = list(idp = 2))
cv_idw_v <- krige.cv(v ~ 1, data, nfold = n_pts, set = list(idp = 2))
rmse_idw_u <- rmse(cv_idw_u@data$observed, cv_idw_u@data$var1.pred)
rmse_idw_v <- rmse(cv_idw_v@data$observed, cv_idw_v@data$var1.pred)
bias_idw_u <- bias(cv_idw_u@data$observed, cv_idw_u@data$var1.pred)
bias_idw_v <- bias(cv_idw_v@data$observed, cv_idw_v@data$var1.pred)

# Validación cruzada Kriging (universal) usando variogramas ajustados:
cv_krig_u <- krige.cv(u ~ x + y, data, model = model_u)
cv_krig_v <- krige.cv(v ~ x + y, data, model = model_v)
rmse_krig_u <- rmse(cv_krig_u@data$observed, cv_krig_u@data$var1.pred)
rmse_krig_v <- rmse(cv_krig_v@data$observed, cv_krig_v@data$var1.pred)
bias_krig_u <- bias(cv_krig_u@data$observed, cv_krig_u@data$var1.pred)
bias_krig_v <- bias(cv_krig_v@data$observed, cv_krig_v@data$var1.pred)

message(sprintf("IDW - RMSE_u=%.3f, RMSE_v=%.3f; Bias_u=%.3f, Bias_v=%.3f",
                rmse_idw_u, rmse_idw_v, bias_idw_u, bias_idw_v))
message(sprintf("Kriging - RMSE_u=%.3f, RMSE_v=%.3f; Bias_u=%.3f, Bias_v=%.3f",
                rmse_krig_u, rmse_krig_v, bias_krig_u, bias_krig_v))

# 7. Calcular lat/long desde UTM para resultados
message("Transformando coordenadas UTM a lat/lon...")
result_sf <- st_as_sf(grid_coords, coords = c("x","y"), crs = 32629)  # usar EPSG 32629 UTM 29N
result_latlon <- st_transform(result_sf, 4326)  # a WGS84 lon/lat
coords_ll <- st_coordinates(result_latlon)
grid_coords$lon <- coords_ll[,1]
grid_coords$lat <- coords_ll[,2]

# Opcional: Calcular velocidad y dirección a partir de u,v interpolados para cada método
grid_coords$vel_idw  <- sqrt(grid_coords$u_idw^2 + grid_coords$v_idw^2)
grid_coords$vel_krig <- sqrt(grid_coords$u_krig^2 + grid_coords$v_krig^2)
# Dirección meteorológica (0°N, rotación horaria) a partir de componentes:
calc_dir <- function(u,v) {
  # Convertir componentes a dirección (grados desde el norte, viento de)
  # Fórmula: dir = atan2(-u, -v) en radianes, luego a grados 0-360
  ang <- atan2(-u, -v) * 180/pi
  ang[ang < 0] <- ang[ang < 0] + 360
  return(ang)
}
grid_coords$dir_idw  <- calc_dir(grid_coords$u_idw, grid_coords$v_idw)
grid_coords$dir_krig <- calc_dir(grid_coords$u_krig, grid_coords$v_krig)

# Añadir columna de fecha para particionado
grid_coords$fecha <- fecha_str

# 8. Guardar resultados en formato Parquet particionado por fecha en S3
message("Guardando resultados en ", output_path, " (particionado por fecha)...")
write_dataset(grid_coords, path = output_path, format = "parquet", partitioning = "fecha")
message("Proceso completado. Datos guardados en Parquet.")
```

**Explicación:** Este script realiza secuencialmente todos los pasos descritos anteriormente de forma automatizada. Se diseñó para ser ejecutado en terminal, por ejemplo:

```bash
Rscript wind_interpolation.R 2025-05-14 5 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados_interpolados/ 5 0.5 5000
```

donde `2025-05-14` es la fecha a procesar, `5` es la resolución en km de la nueva malla, y se indican los paths de S3 para leer los Parquet de entrada y escribir los resultados.

## 8. Scripts Bash para ejecución y configuración AWS

Por último, se proveen dos scripts Bash para facilitar la ejecución del flujo y la configuración de recursos AWS necesarios:

### Script Bash: Ejecución local del script R

Este script `run_interpolation.sh` permite lanzar la interpolación desde la línea de comandos de forma sencilla, pasando parámetros y asegurando que el perfil y región de AWS estén configurados para que R (Arrow) pueda acceder a S3. Por defecto usa el perfil `default` y región `eu-west-3` si no se especifican otros.

```bash
#!/bin/bash
# run_interpolation.sh: Ejecuta el script R de interpolación con los parámetros dados.

# Valores por defecto
PROFILE="default"
REGION="eu-west-3"

# Leer opciones -p (profile) y -r (region) si se proporcionan
while [[ $# -gt 0 ]]; do
  key="$1"
  case $key in
    -p|--profile)
      PROFILE="$2"
      shift; shift ;;
    -r|--region)
      REGION="$2"
      shift; shift ;;
    *)
      # El resto de argumentos se interpretan como <fecha> <resolucion_km> <input_path> <output_path>
      break ;;
  esac
done

# A continuación, los argumentos posicionales esperados:
FECHA="$1"
RESOLUCION="$2"
INPUT_PATH="$3"
OUTPUT_PATH="$4"

if [[ -z "$FECHA" || -z "$RESOLUCION" || -z "$INPUT_PATH" || -z "$OUTPUT_PATH" ]]; then
  echo "Uso: $0 [-p profile] [-r region] <fecha> <res_km> <input_s3_path> <output_s3_path>"
  exit 1
fi

# Exportar variables de entorno AWS para que Arrow en R las use
export AWS_PROFILE="$PROFILE"
export AWS_DEFAULT_REGION="$REGION"

echo "Ejecutando interpolación para fecha $FECHA con resolución ${RESOLUCION}km..."
Rscript wind_interpolation.R "$FECHA" "$RESOLUCION" "$INPUT_PATH" "$OUTPUT_PATH"

if [[ $? -eq 0 ]]; then
  echo "Interpolación completada con éxito."
else
  echo "Hubo un error en la ejecución de la interpolación." >&2
fi
```

**Uso de este script:** por ejemplo:

```bash
./run_interpolation.sh -p default -r eu-west-3 2025-05-14 5 s3://mi-bucket/datos/ s3://mi-bucket/resultados/
```

Esto selecciona el perfil AWS `default` y la región `eu-west-3`, luego ejecuta el Rscript con fecha 2025-05-14, resolución 5 km, leyendo datos de `s3://mi-bucket/datos/` y guardando en `s3://mi-bucket/resultados/`. El script exporta `AWS_PROFILE` y `AWS_DEFAULT_REGION` para que la librería Arrow de R use esas credenciales automáticamente.

### Script Bash: Configuración de bucket S3 y roles IAM

Este script `setup_aws_resources.sh` crea en AWS los recursos necesarios: un bucket S3 para almacenar datos/resultados si no existe, y un rol de IAM con permisos de acceso a S3. Se puede personalizar el nombre de bucket y rol. Requiere AWS CLI configurado con un perfil que tenga permisos para crear recursos.

```bash
#!/bin/bash
# setup_aws_resources.sh: Crea bucket S3 y rol IAM con permisos S3.

PROFILE="default"
REGION="eu-west-3"
BUCKET=""
ROLE_NAME="WindInterpolationRole"

# Parsear opciones -p, -r, -b, -n
while [[ $# -gt 0 ]]; do
  case "$1" in
    -p|--profile)
      PROFILE="$2"; shift 2 ;;
    -r|--region)
      REGION="$2"; shift 2 ;;
    -b|--bucket)
      BUCKET="$2"; shift 2 ;;
    -n|--role-name)
      ROLE_NAME="$2"; shift 2 ;;
    *)
      echo "Uso: $0 [-p profile] [-r region] -b <bucket_name> [-n role_name]"
      exit 1 ;;
  esac
done

if [[ -z "$BUCKET" ]]; then
  echo "Debe especificar un nombre de bucket con -b"
  exit 1
fi

echo "Creando bucket S3 '$BUCKET' en region $REGION (si no existe)..."
aws s3api head-bucket --bucket "$BUCKET" --profile "$PROFILE" 2>/dev/null
if [[ $? -ne 0 ]]; then
  aws s3 mb "s3://$BUCKET" --region "$REGION" --profile "$PROFILE"
  echo "Bucket $BUCKET creado."
else
  echo "Bucket $BUCKET ya existe (o acceso verificado)."
fi

echo "Creando rol IAM '$ROLE_NAME' con permisos de acceso a $BUCKET..."
# Documento de confianza para que EC2 (u otro servicio) asuma el rol
read -r -d '' TRUST_POLICY << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": { "Service": "ec2.amazonaws.com" },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role --role-name "$ROLE_NAME" \
    --assume-role-policy-document "$TRUST_POLICY" \
    --profile "$PROFILE"

# Crear política con acceso limitado al bucket
POLICY_NAME="${ROLE_NAME}S3Access"
read -r -d '' POLICY_DOC << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [ "s3:ListBucket" ],
      "Resource": "arn:aws:s3:::$BUCKET"
    },
    {
      "Effect": "Allow",
      "Action": [ "s3:GetObject", "s3:PutObject" ],
      "Resource": "arn:aws:s3:::$BUCKET/*"
    }
  ]
}
EOF

aws iam put-role-policy --role-name "$ROLE_NAME" \
    --policy-name "$POLICY_NAME" \
    --policy-document "$POLICY_DOC" \
    --profile "$PROFILE"

echo "Rol $ROLE_NAME creado y política $POLICY_NAME adjuntada."
echo "Ahora puede asignar este rol a una instancia EC2 u otro servicio que ejecute los scripts R, para acceso a S3 sin credenciales."
```

**Descripción:** Este script, dado un nombre de bucket, verifica si existe o lo crea. Luego crea un rol IAM (`WindInterpolationRole` por defecto) con una **trust policy** que permite a EC2 asumirlo (esto se puede modificar para Lambda u otros, cambiando el principal). Adjunta una política en línea que permite listar el bucket especificado y leer/escribir objetos dentro de él (acciones S3 GetObject, PutObject, ListBucket limitadas al bucket). De este modo, cualquier entidad (p. ej. una instancia EC2) con este rol podrá acceder al bucket para leer los Parquet de entrada y escribir los resultados sin necesidad de credenciales explícitas.

**Uso del script:** por ejemplo:

```bash
./setup_aws_resources.sh -p default -r eu-west-3 -b mi-bucket -n WindInterpolationRole
```

Esto configura en la cuenta AWS (perfil `default`) el bucket `mi-bucket` en región París (`eu-west-3`) y crea el rol `WindInterpolationRole` con permisos de lectura/escritura en `mi-bucket`. Posteriormente, habría que lanzar la instancia o servicio con ese rol para que los scripts R funcionen sin proporcionar claves. Si se ejecuta localmente, basta con que las credenciales del perfil tengan acceso al bucket, o usar las variables de entorno `AWS_PROFILE` como se hace en `run_interpolation.sh`.

---

**Resumen:** Con estos pasos y scripts, se logra un flujo reproducible completamente por línea de comandos: conversión de datos meteorológicos a un formato columnar (Parquet), interpolación espacial del viento mediante dos métodos (IDW y regression-kriging) con cálculo de métricas de error (RMSE, sesgo), transformación de coordenadas a geográficas, y almacenaje eficiente de resultados en S3. Toda la solución está orientada a ejecución automatizada (scripts R y Bash) sin componentes gráficos, facilitando su integración en entornos de producción o pipelines de datos.
