Perfecto. Implementaré en la carpeta `scripts/` del repositorio `GOFUVI/hf-eolus-interpolation-toolkit` los archivos necesarios para ejecutar la arquitectura de interpolación con R. Esto incluirá:

* Scripts R para conversión y procesamiento (`wind_interpolation.R`, etc.)
* Script Bash para ejecución desde CLI con parámetros (`run_interpolation.sh`)
* Script Bash para provisión de recursos AWS (`setup_aws_resources.sh`)
* Dockerfile con entorno reproducible de R
* Un archivo `README.md` con instrucciones claras de uso, dependencias y ejemplos de ejecución

Te avisaré cuando esté listo para revisión.


# Implementación de la Arquitectura de Interpolación de Viento

En esta sección se presentan todos los archivos necesarios en la carpeta `scripts/` del repositorio `GOFUVI/hf-eolus-interpolation-toolkit` para ejecutar la arquitectura de interpolación espacial de viento con R descrita. La solución incluye:

* Un **script R principal** `wind_interpolation.R` que realiza la lectura de datos desde Parquet (S3), conversión de velocidad/dirección a componentes U/V, interpolación espacial mediante IDW y Regression-Kriging, cálculo de métricas RSR y Bias por validación cruzada, transformación de coordenadas UTM a geográficas, y escritura de los resultados en Parquet en S3 particionado por fecha.
* Un **script Bash** `run_interpolation.sh` para lanzar la ejecución del script R, permitiendo especificar fecha, factor de resolución, rutas S3 y el perfil/región de AWS (perfil por defecto `default`, región `eu-west-3`), exportando las variables de entorno necesarias para que **Apache Arrow** en R pueda acceder a S3.
* Un **script Bash** `setup_aws_resources.sh` para configurar los recursos AWS necesarios: crea un bucket S3 si no existe, y un rol IAM con política de acceso limitada a dicho bucket (lectura/escritura).
* Un **Dockerfile** que prepara un entorno con R y todos los paquetes requeridos: `phylin`, `gstat`, `Metrics`, `arrow`, `sf`, `sp`, etc., incluyendo las dependencias del sistema necesarias para paquetes geoespaciales.
* Un archivo **README** con instrucciones de uso: instalación de dependencias (o construcción de la imagen Docker), ejemplos de ejecución y requisitos de configuración de AWS (perfil de AWS CLI o rol IAM configurado).

A continuación, se detallan cada uno de estos componentes:

## Script R principal: `wind_interpolation.R`

Este script R realiza todo el flujo de interpolación de viento. Acepta parámetros por línea de comandos: `<fecha> <res_factor> <input_path> <output_path>`. Sus pasos principales son:

* **Lectura de datos**: Lee desde la ubicación de entrada (posiblemente un bucket S3) archivos Parquet con los datos de viento (incluyendo columnas de velocidad, dirección, coordenadas UTM x/y, etc.). Si los datos están particionados por fecha, filtra por la fecha dada.
* **Conversión a componentes U/V**: Convierte los datos de viento de representación dirección+velocidad a componentes cartesianas **u** (este-oeste) y **v** (norte-sur) usando la convención meteorológica estándar: `u = -V * sin(θ)` y `v = -V * cos(θ)`, donde *V* es la velocidad y *θ* la dirección en radianes (0° = viento del norte, 90° = viento del este, etc.).
* **Generación de malla de salida**: Construye una nueva grilla de puntos en coordenadas UTM usando un factor de resolución en lugar de la resolución en kilómetros. Se calcula la resolución base del grid original a partir de la mínima distancia entre valores únicos de x/y, y el paso de la malla se define como `base_res / res_factor`. Por ejemplo, un factor de 2 inserta un nodo entre cada par de nodos, un factor de 3 inserta dos nodos, etc. La grilla cubre el dominio usando las extensiones mínima y máxima de x e y (alineadas al grid base).
* **Interpolación espacial**: Aplica dos métodos de interpolación sobre las componentes *u* y *v*:

  * *IDW (Inverse Distance Weighting)*: Utiliza la función `phylin::idw()` para interpolar *u* y *v* por separado a la malla de salida, con potencia inversa 2 por defecto.
  * *Regression-Kriging (Kriging con deriva)*: Ajusta un modelo de tendencia (deriva) `u ~ x + y` (y similar para *v*) y luego realiza kriging de los residuos usando el paquete **gstat**. Se calcula el variograma experimental y se ajusta un modelo (por ejemplo, esférico) con `fit.variogram`, luego se obtiene la predicción kriging en los puntos de la malla.
* **Validación cruzada y métricas**: Emplea validación cruzada *leave-one-out* para estimar el error de cada método. Con `gstat::krige.cv` se obtienen predicciones omitendo cada punto a su vez tanto para IDW (especificando `set = list(idp=2)` para usar IDW en `krige.cv`) como para kriging. A partir de estas predicciones cruzadas se calculan las métricas **RMSE** (root mean square error) y **Bias** (sesgo medio) usando el paquete **Metrics** (`rmse(observados, predichos)` y `bias(observados, predichos)`), para *u* y *v* con ambos métodos. Un buen método presentará RMSE bajo y bias cercano a 0 (sin sobre- ni sub-estimaciones sistemáticas).
* **Transformación de coordenadas**: Convierte las coordenadas de los puntos de resultado desde UTM (p.ej. zona 29N, WGS84) a latitud/longitud (EPSG:4326) usando el paquete **sf** (`st_as_sf` + `st_transform`), agregando columnas `lon` y `lat` a los resultados.
* **Escritura de resultados**: Escribe el data frame de resultados (incluyendo componentes interpolados, velocidad y dirección interpoladas derivadas opcionalmente, coordenadas UTM y geográficas, y la fecha) en formato **Parquet** en la ruta de salida S3, particionando por la columna de fecha. Para ello se utiliza `arrow::write_dataset(..., partitioning = "fecha")`, lo que creará subdirectorios por cada valor de fecha (por ejemplo, `.../fecha=2025-05-14/part-00001-...parquet`).

A continuación se muestra el contenido del archivo `wind_interpolation.R` con la implementación descrita:

```r
# wind_interpolation.R
# Carga de librerías necesarias
library(arrow)    # para leer/escribir Parquet y acceso a S3
library(phylin)   # para interpolación IDW
library(gstat)    # para variogramas y kriging
library(sf)       # para transformación de coordenadas (UTM a lat/long)
library(Metrics)  # para calcular RMSE y bias

# Leer argumentos de entrada (fecha, resolución, rutas S3)
args <- commandArgs(trailingOnly = TRUE)
if(length(args) < 7) {
  stop(
    "Uso: Rscript wind_interpolation.R <fecha> <res_factor> <input_path> <output_path> \
<cutoff_km> <width_km> <subsample_pct>"
  )
}
fecha_str   <- args[1]             # e.g. "2025-05-14"
res_km      <- as.numeric(args[2]) # e.g. 5 (km)
input_path  <- args[3]             # e.g. "s3://bucket/datos_parquet/"
output_path <- args[4]             # e.g. "s3://bucket/resultados/"

# 1. Leer datos de entrada desde Parquet (filtrando por fecha si aplica)
message("Leyendo datos de entrada de ", input_path)
ds <- open_dataset(input_path)
# Si la tabla está particionada por fecha, filtrar antes de collect:
# data <- ds %>% filter(fecha == fecha_str) %>% collect()
# En este ejemplo, leemos todo y filtramos en R si existe columna 'fecha'.
data <- collect(ds)
if("fecha" %in% names(data)) {
  data <- subset(data, fecha == fecha_str)
}
n_pts <- nrow(data)
message("Puntos originales cargados: ", n_pts)

# 2. Asegurarse de que existen columnas u y v (si no, calcularlas)
if(!("u" %in% names(data) && "v" %in% names(data))) {
  message("Calculando componentes u/v a partir de velocidad/direccion...")
  deg2rad <- pi/180
  data$u <- - data$velocidad * sin(data$direccion * deg2rad)
  data$v <- - data$velocidad * cos(data$direccion * deg2rad)
}

# 3. Construir la malla de salida usando un factor de resolución
min_x <- floor(min(data$x)); max_x <- ceiling(max(data$x))
min_y <- floor(min(data$y)); max_y <- ceiling(max(data$y))
# Calcular resolución base y generar secuencias con paso = base_res / res_factor
unique_x <- sort(unique(data$x))
unique_y <- sort(unique(data$y))
base_res <- min(min(diff(unique_x)), min(diff(unique_y)))
# Alinear dominio a la grilla base
min_x <- floor(min(data$x) / base_res) * base_res
max_x <- ceiling(max(data$x) / base_res) * base_res
min_y <- floor(min(data$y) / base_res) * base_res
max_y <- ceiling(max(data$y) / base_res) * base_res
grid_spacing <- base_res / res_factor
grid_x <- seq(min_x, max_x, by = grid_spacing)
grid_y <- seq(min_y, max_y, by = grid_spacing)
grid_coords <- expand.grid(x = grid_x, y = grid_y)
message("Malla de salida generada: ", nrow(grid_coords),
        " puntos (factor = ", res_factor,
        ", spacing = ", signif(grid_spacing, 6), " km)")

# 4. Interpolación IDW para u y v
message("Interpolando con IDW...")
idw_u <- idw(values = data$u, coords = data[, c("x","y")], grid = grid_coords)  # vector resultante
idw_v <- idw(values = data$v, coords = data[, c("x","y")], grid = grid_coords)
# Añadir resultados IDW al data.frame de la grilla
grid_coords$u_idw <- idw_u
grid_coords$v_idw <- idw_v

# 5. Interpolación por Regression-Kriging (usando gstat)
message("Ajustando variogramas y ejecutando kriging...")
library(sp)  # usar SpatialPoints/CRS de sp para gstat
# Convertir datos a objeto espacial (SpatialPointsDataFrame)
coordinates(data) <- ~ x + y
# Asumir proyección UTM 29N (WGS84). NOTA: unidades en kilómetros si los datos x,y están en km.
proj4string(data) <- CRS("+proj=utm +zone=29 +datum=WGS84 +units=km +no_defs")
# Crear objeto spatial para los puntos de la grilla
gridded_points <- SpatialPoints(grid_coords[, c("x","y")], proj4string = CRS(proj4string(data)))

# Variograma y kriging para u
vgm_u <- variogram(u ~ x + y, data)                 # variograma de residuos de u (tendencia ~ x+y)
model_u <- fit.variogram(vgm_u, vgm(model="Sph"))   # ajustar modelo esférico al variograma
krig_u <- krige(u ~ x + y, data, newdata = gridded_points, model = model_u)
grid_coords$u_krig <- krig_u$var1.pred  # predicción interpolada de u

# Variograma y kriging para v
vgm_v <- variogram(v ~ x + y, data)
model_v <- fit.variogram(vgm_v, vgm(model="Sph"))
krig_v <- krige(v ~ x + y, data, newdata = gridded_points, model = model_v)
grid_coords$v_krig <- krig_v$var1.pred

# 6. Calcular métricas de evaluación (LOOCV RSR y Bias)
message("Calculando RSR y bias por validacion cruzada (LOOCV)...")
# Validación cruzada IDW usando krige.cv (idp=2 para IDW)
cv_idw_u <- krige.cv(u ~ 1, data, nfold = n_pts, set = list(idp = 2))
cv_idw_v <- krige.cv(v ~ 1, data, nfold = n_pts, set = list(idp = 2))
# Calculate RSR and bias (LOOCV)
rsr_idw_u <- rmse(cv_idw_u@data$observed, cv_idw_u@data$var1.pred) / sd(cv_idw_u@data$observed)
rsr_idw_v <- rmse(cv_idw_v@data$observed, cv_idw_v@data$var1.pred) / sd(cv_idw_v@data$observed)
bias_idw_u <- bias(cv_idw_u@data$observed, cv_idw_u@data$var1.pred)
bias_idw_v <- bias(cv_idw_v@data$observed, cv_idw_v@data$var1.pred)

# Validación cruzada Kriging (utilizando modelos variográficos ajustados)
cv_krig_u <- krige.cv(u ~ x + y, data, model = model_u)
cv_krig_v <- krige.cv(v ~ x + y, data, model = model_v)
# Calculate RSR and bias (LOOCV)
rsr_krig_u <- rmse(cv_krig_u@data$observed, cv_krig_u@data$var1.pred) / sd(cv_krig_u@data$observed)
rsr_krig_v <- rmse(cv_krig_v@data$observed, cv_krig_v@data$var1.pred) / sd(cv_krig_v@data$observed)
bias_krig_u <- bias(cv_krig_u@data$observed, cv_krig_u@data$var1.pred)
bias_krig_v <- bias(cv_krig_v@data$observed, cv_krig_v@data$var1.pred)

# Mostrar métricas calculadas
message(sprintf("IDW - RSR_u=%.3f, RSR_v=%.3f; Bias_u=%.3f, Bias_v=%.3f",
                rsr_idw_u, rsr_idw_v, bias_idw_u, bias_idw_v))
message(sprintf("Kriging - RSR_u=%.3f, RSR_v=%.3f; Bias_u=%.3f, Bias_v=%.3f",
                rsr_krig_u, rsr_krig_v, bias_krig_u, bias_krig_v))

# 7. Transformar coordenadas UTM a lat/lon (WGS84)
message("Transformando coordenadas UTM a lat/lon...")
result_sf <- st_as_sf(grid_coords, coords = c("x","y"), crs = 32629)  # definir CRS UTM 29N (EPSG:32629)
result_latlon <- st_transform(result_sf, 4326)  # transformar a WGS84 (EPSG:4326)
coords_ll <- st_coordinates(result_latlon)
grid_coords$lon <- coords_ll[,1]
grid_coords$lat <- coords_ll[,2]

# Opcional: Calcular velocidad y dirección a partir de u,v interpolados para cada método
grid_coords$vel_idw  <- sqrt(grid_coords$u_idw^2 + grid_coords$v_idw^2)
grid_coords$vel_krig <- sqrt(grid_coords$u_krig^2 + grid_coords$v_krig^2)
# Calcular dirección (origen del viento, 0° = Norte, sentido horario)
calc_dir <- function(u,v) {
  ang <- atan2(-u, -v) * 180/pi       # dirección en grados tomando -u,-v (origen del viento)
  ang[ang < 0] <- ang[ang < 0] + 360  # ajustar rango a [0,360)
  return(ang)
}
grid_coords$dir_idw  <- calc_dir(grid_coords$u_idw, grid_coords$v_idw)
grid_coords$dir_krig <- calc_dir(grid_coords$u_krig, grid_coords$v_krig)

# Añadir columna de fecha (para particionado por fecha en S3)
grid_coords$fecha <- fecha_str

# 8. Guardar resultados en formato Parquet particionado por fecha en S3
message("Guardando resultados en ", output_path, " (particionado por fecha)...")
write_dataset(grid_coords, path = output_path, format = "parquet", partitioning = "fecha")
message("Proceso completado. Datos guardados en Parquet.")
```

**Nota:** Este script asume que los datos de entrada Parquet contienen al menos las columnas `x`, `y` (coordenadas en UTM, unidades en kilómetros), `velocidad` (magnitud del viento, e.g. m/s) y `direccion` (dirección del viento en grados, meteorológica, es decir, grados desde el norte, sentido horario). Si además incluye una columna de fecha (por ejemplo `fecha` con formato `"YYYY-MM-DD"`), se utiliza para filtrar los datos y para particionar la salida. Asegúrese de que la proyección UTM y unidades sean correctas (en este caso se usa UTM zona 29N WGS84, asumiendo datos de Galicia, con `x`, `y` en kilómetros; si estuvieran en metros habría que convertir a km o ajustar la definición de CRS).

## Script Bash `run_interpolation.sh`

Este script Bash facilita la ejecución del flujo de interpolación. Recibe parámetros para especificar el perfil de AWS y región (opcionales) y luego los argumentos principales (fecha, resolución, ruta de datos de entrada y ruta de resultados). Su funcionalidad es:

* Define perfil AWS (`-p` o `--profile`) y región (`-r` o `--region`) con valores por defecto `default` y `eu-west-3` respectivamente. Estos corresponden a un perfil de credenciales configurado en AWS CLI y a la región donde se ubican los recursos (bucket S3, etc.).
* Parsea los argumentos posicionales requeridos: `<FECHA>` (fecha a procesar, formato YYYY-MM-DD), `<RESOLUCION>` (tamaño de celda de la malla en km), `<INPUT_PATH>` (ruta S3 de los datos de entrada en Parquet) y `<OUTPUT_PATH>` (ruta S3 donde guardar resultados).
* Exporta las variables de entorno `AWS_PROFILE` y `AWS_DEFAULT_REGION` antes de ejecutar R, de modo que la librería **Arrow** en R pueda utilizar esas credenciales automáticamente para acceder a S3.
* Llama a `Rscript wind_interpolation.R` con los parámetros proporcionados. Si la ejecución finaliza con éxito, muestra un mensaje de confirmación; si ocurre un error, informa del fallo.

Contenido de `run_interpolation.sh`:

```bash
#!/bin/bash
# run_interpolation.sh: Ejecuta el script R de interpolación con los parámetros dados.

# Valores por defecto
PROFILE="default"
REGION="eu-west-3"

# Leer opciones -p (perfil) y -r (región) si se proporcionan
while [[ $# -gt 0 ]]; do
  key="$1"
  case $key in
    -p|--profile)
      PROFILE="$2"
      shift; shift ;;
    -r|--region)
      REGION="$2"
      shift; shift ;;
    *)
      # El resto de argumentos se interpretan como <fecha> <resolucion_km> <input_path> <output_path>
      break ;;
  esac
done

# Argumentos posicionales esperados
FECHA="$1"
RESOLUCION="$2"
INPUT_PATH="$3"
OUTPUT_PATH="$4"

if [[ -z "$FECHA" || -z "$RESOLUCION" || -z "$INPUT_PATH" || -z "$OUTPUT_PATH" ]]; then
  echo "Uso: $0 [-p perfil] [-r region] <fecha> <res_factor> <input_s3_path> <output_s3_path>"
  exit 1
fi

# Exportar variables de entorno AWS para que Arrow (R) acceda a S3
export AWS_PROFILE="$PROFILE"
export AWS_DEFAULT_REGION="$REGION"

echo "Ejecutando interpolación para fecha $FECHA con factor de resolución ${RESOLUCION}, cutoff $CUTOFF km, width $WIDTH km, subsample ${SUBSAMPLE_PCT}%..."
Rscript scripts/wind_interpolation.R "$FECHA" "$RESOLUCION" "$INPUT_PATH" "$OUTPUT_PATH" "$CUTOFF" "$WIDTH" "$SUBSAMPLE_PCT"

if [[ $? -eq 0 ]]; then
  echo "Interpolación completada con éxito."
else
  echo "Hubo un error en la ejecución de la interpolación." >&2
fi
```

**Ejemplo de uso:**

```bash
./run_interpolation.sh -p default -r eu-west-3 -c 5 -w 0.5 -n 10 2025-05-14 \
  2 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados_interpolados/
```
En este ejemplo, se utiliza el perfil AWS `default` y región `eu-west-3`, luego se ejecuta la interpolación para la fecha **2025-05-14** con un factor de resolución de **2**, leyendo los datos desde `s3://mi-bucket/datos_parquet/` y escribiendo los resultados en `s3://mi-bucket/resultados_interpolados/`. El script exportará las variables de entorno necesarias para que R Arrow pueda acceder a S3 con el perfil dado.

## Script Bash `setup_aws_resources.sh`

Este script Bash configura recursos de AWS útiles para la aplicación, en particular el almacenamiento en S3 y permisos de acceso:

* **Creación de bucket S3**: Recibe el nombre de bucket mediante la opción `-b <bucket_name>`. Verifica si el bucket existe intentando leer sus metadatos (`aws s3api head-bucket`). Si no existe, lo crea en la región especificada usando `aws s3 mb`.
* **Creación de rol IAM**: Crea un rol IAM (nombre por defecto `WindInterpolationRole`, modificable con `-n <role_name>`) con una política de confianza que permite a servicios de EC2 asumir el rol. Esto se realiza mediante `aws iam create-role` proporcionando un **trust policy** en formato JSON.
* **Política de acceso S3**: Define y adjunta una política en línea al rol creado, otorgándole permisos **limitados al bucket especificado**: permiso de listar el bucket (ListBucket) y obtener/poner objetos dentro del bucket (GetObject, PutObject). Esta política restringe el acceso únicamente a ese bucket en particular (usando ARN del bucket y de los objetos dentro).
* Los parámetros `-p` (perfil AWS CLI) y `-r` (región) también son aceptados, con valores por defecto `default` y `eu-west-3`, respectivamente, de forma similar al script anterior. Se requiere que la AWS CLI esté configurada con credenciales que tengan privilegios para crear buckets y roles/policies (por ejemplo, un usuario administrador o con permisos específicos en IAM y S3).

Contenido de `setup_aws_resources.sh`:

```bash
#!/bin/bash
# setup_aws_resources.sh: Crea bucket S3 y rol IAM con permisos de acceso a dicho bucket.

PROFILE="default"
REGION="eu-west-3"
BUCKET=""
ROLE_NAME="WindInterpolationRole"

# Parsear opciones -p, -r, -b, -n
while [[ $# -gt 0 ]]; do
  case "$1" in
    -p|--profile)
      PROFILE="$2"; shift 2 ;;
    -r|--region)
      REGION="$2"; shift 2 ;;
    -b|--bucket)
      BUCKET="$2"; shift 2 ;;
    -n|--role-name)
      ROLE_NAME="$2"; shift 2 ;;
    *)
      echo "Uso: $0 [-p perfil] [-r region] -b <bucket_name> [-n role_name]"
      exit 1 ;;
  esac
done

if [[ -z "$BUCKET" ]]; then
  echo "Debe especificar un nombre de bucket con -b"
  exit 1
fi

echo "Creando bucket S3 '$BUCKET' en región $REGION (si no existe)..."
aws s3api head-bucket --bucket "$BUCKET" --profile "$PROFILE" 2>/dev/null
if [[ $? -ne 0 ]]; then
  aws s3 mb "s3://$BUCKET" --region "$REGION" --profile "$PROFILE"
  echo "Bucket $BUCKET creado."
else
  echo "Bucket $BUCKET ya existe o es accesible."
fi

echo "Creando rol IAM '$ROLE_NAME' con permisos de acceso a $BUCKET..."
# Definir la política de confianza para permitir a EC2 asumir el rol
read -r -d '' TRUST_POLICY << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": { "Service": "ec2.amazonaws.com" },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role --role-name "$ROLE_NAME" \
    --assume-role-policy-document "$TRUST_POLICY" \
    --profile "$PROFILE"

# Crear política de acceso S3 limitada al bucket
POLICY_NAME="${ROLE_NAME}S3Access"
read -r -d '' POLICY_DOC << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [ "s3:ListBucket" ],
      "Resource": "arn:aws:s3:::$BUCKET"
    },
    {
      "Effect": "Allow",
      "Action": [ "s3:GetObject", "s3:PutObject" ],
      "Resource": "arn:aws:s3:::$BUCKET/*"
    }
  ]
}
EOF

aws iam put-role-policy --role-name "$ROLE_NAME" \
    --policy-name "$POLICY_NAME" \
    --policy-document "$POLICY_DOC" \
    --profile "$PROFILE"

echo "Rol $ROLE_NAME creado y política $POLICY_NAME adjuntada."
echo "Puede asignar este rol a instancias EC2 u otros servicios para acceso a S3 sin credenciales explícitas."
```

**Ejemplo de uso:** para crear un bucket y rol nuevos, ejecutar:

```bash
./setup_aws_resources.sh -p default -r eu-west-3 -b mi-bucket -n WindInterpolationRole
```

El comando anterior, con perfil `default`, creará el bucket **`mi-bucket`** en la región **`eu-west-3`** (si aún no existe), y un rol IAM llamado **`WindInterpolationRole`** con permisos de lectura/escritura sobre ese bucket. Una vez creado el rol, se puede asignar a una instancia EC2 (u otro servicio AWS) para que los scripts R puedan acceder al bucket **sin necesidad de credenciales AWS explícitas** (gracias a los metadatos/rol de la instancia). Si la ejecución va a ser local, se puede omitir el uso de roles, usando simplemente las credenciales del perfil configurado (el script de ejecución local `run_interpolation.sh` ya manejará las credenciales del perfil AWS especificado).

## Dockerfile (Entorno de R con dependencias)

El siguiente Dockerfile configura un contenedor con R instalado y todos los paquetes necesarios (`phylin`, `gstat`, `Metrics`, `arrow`, `sf`, `sp`, etc.) para poder ejecutar el script de interpolación en cualquier entorno sin tener que instalar manualmente las dependencias. Incluye además las librerías de sistema necesarias para que ciertos paquetes de R (especialmente los geoespaciales como **sf**/**sp** y **arrow**) puedan compilar o funcionar correctamente (por ejemplo, GDAL, PROJ, GEOS, etc.).

Contenido del `Dockerfile`:

```Dockerfile
# Usar una imagen base de R oficial (por ejemplo, R 4.2 en Debian)
FROM r-base:4.2.2

# Instalar librerías de sistema necesarias (espacio, Arrow, etc.)
RUN apt-get update -qq && apt-get install -y --no-install-recommends \
    libcurl4-openssl-dev libssl-dev libxml2-dev \
    libgdal-dev libgeos-dev libproj-dev libudunits2-dev \
    && rm -rf /var/lib/apt/lists/*

# Instalar paquetes de R requeridos
RUN R -e "install.packages(c('phylin','gstat','Metrics','arrow','sf','sp'), repos='http://cran.r-project.org/')"

# Crear directorio de trabajo y copiar los scripts al contenedor
WORKDIR /app
COPY scripts/ ./scripts/

# Establecer la carpeta de trabajo por defecto
WORKDIR /app/scripts

# Comando de entrada por defecto (opcional, se puede omitir para usar directamente `docker run` con comandos)
# CMD ["bash"]
```

**Notas sobre esta imagen:**

* Se usa una imagen base de R oficial. Alternativamente, se podría usar una imagen de Rocker especializada, por ejemplo `rocker/geospatial`, que ya incluye soporte para muchos paquetes geoespaciales.

* Se instalan librerías de sistema: `libgdal-dev`, `libgeos-dev`, `libproj-dev`, etc., necesarias para compilar e instalar el paquete **sf** (que depende de GDAL/GEOS/PROJ) y **sp**. También se instalan dependencias de Arrow y otras (`libcurl4-openssl-dev`, `libssl-dev`, etc. para soporte HTTP/S3 en arrow).

* Luego se instalan todos los paquetes de R mencionados de CRAN. Esto incluirá también sus dependencias (por ejemplo, `sf` traerá `rgdal` o usa `gdal` del sistema, etc.).

* Se copian los scripts del repositorio al directorio `/app/scripts` dentro del contenedor. De este modo, la imagen contiene `wind_interpolation.R` y los bash scripts, listos para ejecutarse.

* El contenedor se puede utilizar ejecutando directamente los scripts. Por ejemplo, una vez construida la imagen (por ej. con `docker build -t wind-interpolation .`), se podría ejecutar:

  ```bash
  docker run --rm -e AWS_PROFILE=default -e AWS_DEFAULT_REGION=eu-west-3 wind-interpolation \
     bash -c "./run_interpolation.sh 2025-05-14 5 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados/"
  ```

  En este ejemplo, se pasa el perfil y región AWS como variables de entorno al contenedor, y luego se llama al script de ejecución. Esto asumirá que en el host local existe la configuración del perfil AWS `default` (normalmente en `~/.aws/credentials`) y que se comparte con el contenedor. **Importante:** Para que el contenedor acceda a las credenciales, puede ser necesario montarlas o configurarlas adecuadamente. Una opción es volcar las credenciales en variables de entorno (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, etc.) en lugar de usar perfiles, o bien ejecutar el contenedor en una instancia EC2 con el rol IAM adecuado adjuntado (en cuyo caso, dentro del contenedor, la IMDS proporcionará credenciales automáticamente).

* Si se prefiere no usar Docker, se puede instalar R y los paquetes necesarios en el sistema host siguiendo el mismo listado de paquetes, o usar RStudio/CRAN para instalarlos. Asegúrese de que la versión de R y los paquetes sean compatibles (por ejemplo, R 4.x).

## Instrucciones de Uso (README)

Esta sección ofrece una guía para instalar las dependencias, configurar AWS y ejecutar el proceso de interpolación de viento paso a paso.

### Prerrequisitos

* **AWS CLI**: Asegúrese de tener la AWS CLI instalada y configurada con credenciales válidas. Si va a ejecutar los scripts en local, necesitará un perfil de AWS configurado (por defecto, usamos `default` en los ejemplos). Las credenciales deben tener permisos para leer/escribir en el bucket S3 que se usará y (si va a usar el script de setup) permisos para crear buckets y roles IAM.
* **Datos de entrada**: Debe contar con los datos meteorológicos de entrada en formato Parquet, almacenados preferentemente en S3. Estos datos deberían incluir las columnas de **velocidad del viento**, **dirección del viento** (grados desde el norte) y coordenadas **x, y** en UTM (misma proyección para todos los puntos, e.g. EPSG 32629 para Galicia). Si los datos provienen de NetCDF originales, asegúrese de convertirlos a Parquet previamente (puede existir un script aparte para conversión NetCDF->Parquet, no incluido aquí). Opcionalmente, incluya una columna de fecha para facilitar la partición.
* **Entorno de ejecución**: Puede ejecutarse en una máquina local con R instalado y los paquetes necesarios, *o bien* usar Docker para aislar el entorno. Si elige Docker, construya la imagen usando el Dockerfile provisto antes de la ejecución (ver instrucciones abajo).

### Instalación de Dependencias

**Opción 1: Entorno local**

1. Instale R (se recomienda R 4.0 o superior).
2. Instale los siguientes paquetes de R desde CRAN: `phylin`, `gstat`, `Metrics`, `arrow`, `sf`, `sp`. Puede hacerlo iniciando R y ejecutando `install.packages(c("phylin","gstat","Metrics","arrow","sf","sp"))`.

   * Asegúrese de tener instaladas librerías de sistema necesarias para compilar estos paquetes. En sistemas Debian/Ubuntu, por ejemplo:

     ```bash
     sudo apt-get install libgdal-dev libgeos-dev libproj-dev libudunits2-dev libcurl4-openssl-dev libssl-dev
     ```

     Esto instalará dependencias para `sf`/`sp` (geospatial) y `arrow` (acceso a S3, etc.).
3. Instale la AWS CLI si no la tiene, y configure un perfil (por ejemplo `default`) con `aws configure`.

**Opción 2: Entorno Docker**

1. Asegúrese de tener Docker instalado.
2. Compile la imagen Docker proporcionada. Desde la raíz del repositorio (donde está el `Dockerfile`), ejecute:

   ```bash
   docker build -t wind-interpolation .
   ```

   Esto construirá una imagen llamada `wind-interpolation` con R y todos los paquetes necesarios.
3. (Opcional) Verifique que la imagen se creó correctamente listando las imágenes con `docker images` o ejecutando una sesión interactiva:

   ```bash
   docker run --rm -it wind-interpolation R --version
   ```

   Debería mostrar la versión de R instalada en el contenedor.

### Configuración de AWS (Bucket y Rol IAM)

Antes de ejecutar la interpolación, se recomienda configurar el almacenamiento en S3 y los permisos adecuados:

* **Bucket S3**: Determine el nombre del bucket donde están/estarán los datos de entrada y resultados. Si el bucket no existe, créelo manualmente en la consola de AWS, o use el script `setup_aws_resources.sh` para crearlo automáticamente.
* **Rol IAM (opcional)**: Si planea ejecutar el proceso en una instancia EC2 (o ECS, Lambda, etc.) sin proporcionar claves de acceso directamente, cree un rol de IAM con permisos al bucket. El script `setup_aws_resources.sh` puede crear dicho rol automáticamente. Si va a ejecutar localmente con AWS CLI, este paso es opcional; puede confiar en las credenciales locales.

Para usar el script automático de configuración, ejecute por ejemplo:

```bash
# Crear bucket y rol usando AWS CLI (requiere credenciales con permisos adecuados)
scripts/setup_aws_resources.sh -p default -r eu-west-3 -b nombre-de-su-bucket -n WindInterpolationRole
```

Este comando usará el perfil `default` y región `eu-west-3` para crear (si no existe) el bucket indicado y un rol IAM llamado "WindInterpolationRole" con permisos limitados a ese bucket. Puede cambiar el nombre del rol con `-n` o omitirlo para usar el predeterminado. Después de correr esto, si está en una instancia EC2, asocie el rol creado a la instancia; si está local, asegúrese de que su perfil AWS tenga acceso al bucket recién creado.

### Ejecución del Proceso de Interpolación

Una vez preparado el entorno y la configuración de AWS:

1. **Verifique los paths S3**: Asegúrese de conocer la ruta S3 donde están los datos de entrada (Parquet). Por ejemplo: `s3://<mi-bucket>/datos_parquet/`. También decida una ruta de salida para resultados, p.ej.: `s3://<mi-bucket>/resultados_interpolados/`. Estas pueden ser la misma ruta de bucket con diferentes prefijos o incluso el mismo prefijo de entrada si se van a escribir en una estructura particionada diferente.

2. **Seleccione la fecha y factor de resolución**: El proceso está pensado para ejecutarse por fecha (p. ej. por fecha de pronóstico o fecha de observación). Determine la fecha a procesar (formato `YYYY-MM-DD`) y el factor de resolución deseado (por ejemplo 2, 3, 4, dependiendo de cuántos nodos adicionales entre cada par de nodos del grid original).

3. **Ejecute el script de interpolación**. Tiene dos formas de hacerlo:

   * **En local (shell)**: Use el script Bash de ejecución:

     ```bash
     scripts/run_interpolation.sh -p default -r eu-west-3  <fecha> <res_factor> <input_s3_path> <output_s3_path>
     ```

     Reemplace `<fecha>` (ej. `2025-05-14`), `<res_factor>` (ej. `2`), `<input_s3_path>` y `<output_s3_path>` por los valores correspondientes a su caso. El ejemplo completo podría ser:

     ```bash
     scripts/run_interpolation.sh -p default -r eu-west-3 2025-05-14 2 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados_interpolados/
     ```

     Este comando utilizará el perfil AWS `default` y la región `eu-west-3` para credenciales, luego invocará `Rscript` con los parámetros. Al finalizar, los resultados interpolados (componentes u/v, velocidad y dirección interpoladas, lat/lon, etc.) se guardarán en el bucket S3 indicado, en subdirectorios por fecha (por ejemplo, `.../fecha=2025-05-14/part-*.parquet`).

   * **En Docker**: Si prefirió usar Docker, puede ejecutar el contenedor con los mismos argumentos. Por ejemplo:

     ```bash
     docker run --rm -e AWS_PROFILE=default -e AWS_DEFAULT_REGION=eu-west-3 wind-interpolation \
       bash -c "scripts/run_interpolation.sh 2025-05-14 2 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados_interpolados/"
     ```

     Esto lanzará un contenedor que ejecuta el script con los parámetros dados. Aquí pasamos las variables de entorno `AWS_PROFILE` y `AWS_DEFAULT_REGION` al contenedor para que Arrow utilice el perfil `default` del host. Tenga en cuenta que para que esto funcione, el contenedor debe tener acceso a las credenciales del perfil (por ejemplo, montando `~/.aws` dentro del contenedor, o exportando `AWS_ACCESS_KEY_ID`/`AWS_SECRET_ACCESS_KEY` en lugar del perfil). Si está en una instancia EC2 con rol, puede ejecutar el contenedor sin variables de entorno y Arrow debería utilizar las credenciales proporcionadas por el rol de la instancia automáticamente.

4. **Monitoree la salida**: El script R irá imprimiendo mensajes (`message()`) indicando el progreso: número de puntos cargados, generación de malla, proceso de interpolación, cálculo de métricas, etc., así como los valores de RMSE y bias para cada método. Esto le permitirá verificar que la interpolación se realizó correctamente y la calidad de la misma.

5. **Verifique los resultados**: Una vez completado, los archivos Parquet resultantes estarán en el bucket S3 de salida, particionados por fecha. Puede usar AWS CLI (`aws s3 ls ...`) para listar el contenido, o utilizar **Apache Arrow** o **AWS Athena/Glue** para leer los Parquet. Cada archivo contendrá las columnas: `x, y` (UTM), `u_idw, v_idw, u_krig, v_krig` (componentes interpolados), `vel_idw, vel_krig` (módulo de viento interpolado), `dir_idw, dir_krig` (dirección de viento interpolada en grados), `lon, lat` (coordenadas geográficas), y la columna de `fecha`.

### Ejemplo completo de flujo

Supongamos que queremos interpolar la fecha 2025-05-14 a una resolución de 5 km, con datos almacenados en `s3://mi-bucket/datos_parquet/` y guardar los resultados en `s3://mi-bucket/resultados_interpolados/`. Además, el bucket S3 y el rol IAM necesarios aún no existen. El flujo sería:

1. Configurar AWS CLI con credenciales (perfil `default` con permisos necesarios).
2. Crear recursos AWS:

   ```bash
   scripts/setup_aws_resources.sh -p default -r eu-west-3 -b mi-bucket -n WindInterpolationRole
   ```

   *(Esto crea el bucket `mi-bucket` y el rol `WindInterpolationRole` con acceso a él.)*
3. Ejecutar la interpolación (puede ser en local o Docker). En local:

   ```bash
   scripts/run_interpolation.sh -p default -r eu-west-3 2025-05-14 5 s3://mi-bucket/datos_parquet/ s3://mi-bucket/resultados_interpolados/
   ```

   *(El script R leerá los Parquet de entrada, realizará la conversión e interpolación, y escribirá los Parquet de salida.)*
4. Verificar la salida:

   ```bash
   aws s3 ls s3://mi-bucket/resultados_interpolados/fecha=2025-05-14/ --profile default --region eu-west-3
   ```

   Debería listar uno o varios archivos Parquet dentro de esa partición de fecha. Puede luego descargar un archivo y examinarlo (por ejemplo con PyArrow, Pandas, etc.) para asegurar que contiene las columnas mencionadas y datos razonables.

Con estos pasos y archivos provistos, se logra una solución completa ejecutable por línea de comandos, que toma datos meteorológicos, los interpola espacialmente con dos métodos distintos, evalúa su desempeño y almacena los resultados de forma eficiente en S3. Esta arquitectura es reproducible y apta para integrarse en pipelines de datos o entornos de producción sin necesidad de intervención manual ni entornos gráficos, cumpliendo con los requisitos establecidos.

**Referencias:**

* Especificaciones y diseño del flujo de interpolación (IDW, kriging, métricas, etc.) del repositorio
* Conversión de velocidad/dirección a componentes u/v
* Interpolación IDW con paquete phylin
* Interpolación por kriging con paquete gstat
* Cálculo de RMSE y Bias con validación cruzada
* Transformación de coordenadas UTM a lat/long con sf
* Escritura de resultados particionados en Parquet con arrow
